# Databricks notebook source
from datetime import datetime
from delta.tables import DeltaTable
from pyspark.sql.functions import (
    input_file_name,
    split,
    lit,
    explode,
    col,
    when,
    concat_ws,
    lower,
    trim,
    udf,
    array,
    expr,
    row_number,
    desc,
    substring,
    concat,
    regexp_replace,
    max,
    explode,
    collect_list,
    monotonically_increasing_id,
    nvl,
    coalesce,
    current_timestamp,
)
from pyspark.sql.window import Window
from pyspark.sql.types import (
    StructType,
    StructField,
    IntegerType,
    StringType,
    ArrayType,
    DoubleType,
)

# COMMAND ----------

# MAGIC %run ../utils/logger

# COMMAND ----------

# MAGIC %run ../utils/read_utility

# COMMAND ----------

# MAGIC %run ../utils/write_utility

# COMMAND ----------

# MAGIC %run ../utils/file_metadata_utility

# COMMAND ----------

try:
    if dbutils:
        pass  # pragma: no cover
except NameError:
    from src.utils.file_metadata_utility import add_epoch_timestamp
    from src.utils.read_utility import read_json, read_delta_table, read_table
    from src.utils.write_utility import write_table

# COMMAND ----------

# pt_gauge variables
PT_TEMPLATE = "(select epoch_timestamp, asset_id from `$catalog`.silver_zone.pt_gauge where $parameter $operator $threshold and asset_id = '$asset_id' and epoch_timestamp between $start_time - $data_frequency and $end_time) condition_$index"

PT_PREFIX = "select min(epoch_timestamp) as start_time, max(epoch_timestamp) as end_time from ( select epoch_timestamp, diff, grp, max(grp) over (order by epoch_timestamp rows between unbounded preceding and current row) group_member from (select epoch_timestamp, diff, case when diff>$data_frequency then sum(diff) OVER (ORDER BY epoch_timestamp ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) end grp from (select epoch_timestamp, coalesce((epoch_timestamp - lag(epoch_timestamp) OVER (ORDER BY epoch_timestamp)), 1) as diff from ("

PT_SUFFIX = ") condition_group) timestamp_diffs) anomaly_group_step_1) anomaly_group group by group_member having max(epoch_timestamp) - min(epoch_timestamp) >= $duration or min(epoch_timestamp) = $start_time - $data_frequency or max(epoch_timestamp) = $end_time"

FM_TEMPLATE = "(select timestamp, asset_id from `$catalog`.silver_zone.flowmeter where $parameter $operator $threshold and asset_id = '$asset_id' and timestamp between $start_time - $data_frequency and $end_time) condition_$index"

FM_PREFIX = "select min(timestamp) as start_time, max(timestamp) as end_time from ( select timestamp, diff, grp, max(grp) over (order by timestamp rows between unbounded preceding and current row) group_member from (select timestamp, diff, case when diff>$data_frequency then sum(diff) OVER (ORDER BY timestamp ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) end grp from (select timestamp, coalesce((timestamp - lag(timestamp) OVER (ORDER BY timestamp)), 1) as diff from ("

FM_SUFFIX = ") condition_group) timestamp_diffs) anomaly_group_step_1) anomaly_group group by group_member having max(timestamp) - min(timestamp) >= $duration or min(timestamp) = $start_time - $data_frequency or max(timestamp) = $end_time"

# no_of_events variables

NO_EVENTS_TEMPLATE = "(select ($start_time + $duration*grp) as start_time, ($start_time + $duration*grp + $duration) as end_time from (select floor((epoch_timestamp - $start_time)/$duration) grp, count(*) count from `$catalog`.silver_zone.microseismic_events where asset_id='$asset_id' and epoch_timestamp between $start_time and $end_time and class=$class group by grp having count(1) $operator $threshold) a ) condition_$index"

NO_EVENTS_TEMPLATE_WITHOUT_CLASS = "(select ($start_time + $duration*grp) as start_time, ($start_time + $duration*grp + $duration) as end_time from (select floor((epoch_timestamp - $start_time)/$duration) grp, count(*) count from `$catalog`.silver_zone.microseismic_events where asset_id='$asset_id' and epoch_timestamp between $start_time and $end_time group by grp having count(1) $operator $threshold) a ) condition_$index"

NO_EVENTS_PREFIX = "select condition_1.start_time, condition_1.end_time from "

# magnitude variables
MAGNITUDE_TEMPLATE = "(select epoch_timestamp from `$catalog`.silver_zone.microseismic_events where asset_id='$asset_id' and class=$class and $parameter $operator $threshold and cast(last_updated_date as bigint) between $start_time and $end_time) condition_$index"

MAGNITUDE_TEMPLATE_WITHOUT_CLASS = "(select epoch_timestamp from `$catalog`.silver_zone.microseismic_events where asset_id='$asset_id' and $parameter $operator $threshold and cast(last_updated_date as bigint) between $start_time and $end_time) condition_$index"

MAGNITUDE_PREFIX = "select condition_1.epoch_timestamp start_time from "

# dss variables

DSS_TEMPLATE_WITHOUT_CLASS_RAW = "(select DISTINCT(`timestamp`) as `timestamp` from `$catalog`.silver_zone.dss where asset_id='$asset_id' and wire = $wire and depth between $length_from and $length_to and $parameter $operator $threshold and cast(`timestamp` as bigint) between $start_time and $end_time) as condition_$index "

DSS_TEMPLATE_WITHOUT_CLASS_FUNCTION = "(select `timestamp` from `$catalog`.silver_zone.dss where asset_id='$asset_id'and wire = $wire and depth between $length_from and $length_to and cast(`timestamp` as bigint) between $start_time and $end_time group by `timestamp` having $function($parameter) $operator $threshold ) as condition_$index"


DSS_PREFIX = "select condition_1.timestamp as start_time from "

# COMMAND ----------

DSS_PARAMETER_MAPPING = {
    "axial_strain": "axial_strain_thermal",
    "bend_magnitude": "bend_mag",
    "dts": "curr_temp",
}
DSS_FUNCTION_MAPPING = {
    "average" : "avg",
    "maximum" : "max",
    "minimum" : "min",
}

# COMMAND ----------

def generate_subquery(
    template: str, asset_id: str, index: int, data_frequency: str = None
) -> str:
    """Generates a subquery by replacing placeholders in the template."""
    query = template.replace("$asset_id", str(asset_id)).replace("$index", str(index))
    if data_frequency is not None:
        query = query.replace("$data_frequency", str(data_frequency))
    return query

# COMMAND ----------

def handle_flow_meter(asset_id_list: list[str], data_frequency_list: list[str]) -> str:
    """Handles query generation for flow meter parameters."""
    if (
        data_frequency_list == None
        or len(data_frequency_list) == 0
        or len(data_frequency_list) != len(asset_id_list)
        or None in data_frequency_list
    ):
        return "data_frequency_null"

    if len(asset_id_list) == 1:
        first_subquery = generate_subquery(
            FM_TEMPLATE, asset_id_list[0], 1, data_frequency_list[0]
        )
        subquery = f"select condition_1.timestamp from {first_subquery}"
        complete_query = f"{FM_PREFIX.replace('$data_frequency', str(data_frequency_list[0]))}{subquery}{FM_SUFFIX.replace('$data_frequency', str(data_frequency_list[0]))}"
    else:
        first_subquery = generate_subquery(
            FM_TEMPLATE, asset_id_list[0], 1, data_frequency_list[0]
        )
        subquery = f"select condition_1.timestamp from {first_subquery}"
        first_complete_query = f"{FM_PREFIX.replace('$data_frequency', str(data_frequency_list[0]))}{subquery}{FM_SUFFIX.replace('$data_frequency', str(data_frequency_list[0]))}) com_query_1 "
        join_template = ") com_query_$index2 on com_query_1.end_time >= com_query_$index2.start_time and com_query_1.start_time <= com_query_$index2.end_time "
        loop_query = first_complete_query
        max_start = ""
        min_end = ""
        for i, asset_id in enumerate(asset_id_list[1:]):
            com_subquery = generate_subquery(
                FM_TEMPLATE, asset_id, 1, data_frequency_list[i + 1]
            )
            i_complete_query = f"{FM_PREFIX.replace('$data_frequency', str(data_frequency_list[i+1]))}{com_subquery}{FM_SUFFIX.replace('$data_frequency', str(data_frequency_list[i+1]))}"
            loop_query += f" inner join ( {i_complete_query}{join_template.replace('$index2', str(i+2))}"
            max_start += f", com_query_{i+2}.start_time"
            min_end += f", com_query_{i+2}.end_time"
        complete_query = f"select start_time, end_time from (select greatest(com_query_1.start_time{max_start}) AS start_time, least(com_query_1.end_time {min_end}) AS end_time from ({loop_query}) greatest_least where end_time - start_time >= $duration"

    return complete_query

# COMMAND ----------

def handle_pressure_temperature(
    asset_id_list: list[str], data_frequency_list: list[str]
) -> str:
    """Handles query generation for 'pressure' or 'temperature' parameters."""
    if (
        data_frequency_list == None
        or len(data_frequency_list) == 0
        or len(data_frequency_list) != len(asset_id_list)
        or None in data_frequency_list
    ):
        return "data_frequency_null"

    if len(asset_id_list) == 1:
        first_subquery = generate_subquery(
            PT_TEMPLATE, asset_id_list[0], 1, data_frequency_list[0]
        )
        subquery = f"select condition_1.epoch_timestamp from {first_subquery}"
        complete_query = f"{PT_PREFIX.replace('$data_frequency', str(data_frequency_list[0]))}{subquery}{PT_SUFFIX.replace('$data_frequency', str(data_frequency_list[0]))}"
    else:
        first_subquery = generate_subquery(
            PT_TEMPLATE, asset_id_list[0], 1, data_frequency_list[0]
        )
        subquery = f"select condition_1.epoch_timestamp from {first_subquery}"
        first_complete_query = f"{PT_PREFIX.replace('$data_frequency', str(data_frequency_list[0]))}{subquery}{PT_SUFFIX.replace('$data_frequency', str(data_frequency_list[0]))}) com_query_1 "
        join_template = ") com_query_$index2 on com_query_1.end_time >= com_query_$index2.start_time and com_query_1.start_time <= com_query_$index2.end_time "
        loop_query = first_complete_query
        max_start = ""
        min_end = ""
        for i, asset_id in enumerate(asset_id_list[1:]):
            com_subquery = generate_subquery(
                PT_TEMPLATE, asset_id, 1, data_frequency_list[i + 1]
            )
            i_complete_query = f"{PT_PREFIX.replace('$data_frequency', str(data_frequency_list[i+1]))}{com_subquery}{PT_SUFFIX.replace('$data_frequency', str(data_frequency_list[i+1]))}"
            loop_query += f" inner join ( {i_complete_query}{join_template.replace('$index2', str(i+2))}"
            max_start += f", com_query_{i+2}.start_time"
            min_end += f", com_query_{i+2}.end_time"
        complete_query = f"select start_time, end_time from (select greatest(com_query_1.start_time{max_start}) AS start_time, least(com_query_1.end_time {min_end}) AS end_time from ({loop_query}) greatest_least where end_time - start_time >= $duration"

    return complete_query

# COMMAND ----------

def handle_no_of_events(asset_id_list: list[str], clss: int) -> str:
    """Handles query generation for 'no_of_events' parameter."""
    if clss is not None:
        first_subquery = generate_subquery(NO_EVENTS_TEMPLATE, asset_id_list[0], 1)
        join_template = " on condition_1.grp = condition_$index2.grp"
        subqueries = [
            f" inner join {generate_subquery(NO_EVENTS_TEMPLATE, asset_id, i+2)}{join_template.replace('$index2', str(i+2))}"
            for i, asset_id in enumerate(asset_id_list[1:])
        ]
    else:
        first_subquery = generate_subquery(
            NO_EVENTS_TEMPLATE_WITHOUT_CLASS, asset_id_list[0], 1
        )
        join_template = " on condition_1.grp = condition_$index2.grp"
        subqueries = [
            f" inner join {generate_subquery(NO_EVENTS_TEMPLATE_WITHOUT_CLASS, asset_id, i+2)}{join_template.replace('$index2', str(i+2))}"
            for i, asset_id in enumerate(asset_id_list[1:])
        ]
    subquery = first_subquery + " ".join(subqueries)
    complete_query = NO_EVENTS_PREFIX + subquery
    return complete_query

# COMMAND ----------

def handle_magnitude(asset_id_list: list[str], clss: int) -> str:
    """Handles query generation for 'magnitude' parameter."""
    if clss is not None:
        first_subquery = generate_subquery(MAGNITUDE_TEMPLATE, asset_id_list[0], 1)
        join_template = (
            " on condition_1.epoch_timestamp = condition_$index2.epoch_timestamp"
        )
        subqueries = [
            f" inner join {generate_subquery(MAGNITUDE_TEMPLATE, asset_id, i+2)}{join_template.replace('$index2', str(i+2))}"
            for i, asset_id in enumerate(asset_id_list[1:])
        ]
    else:
        first_subquery = generate_subquery(
            MAGNITUDE_TEMPLATE_WITHOUT_CLASS, asset_id_list[0], 1
        )
        join_template = (
            " on condition_1.epoch_timestamp = condition_$index2.epoch_timestamp"
        )
        subqueries = [
            f" inner join {generate_subquery(MAGNITUDE_TEMPLATE_WITHOUT_CLASS, asset_id, i+2)}{join_template.replace('$index2', str(i+2))}"
            for i, asset_id in enumerate(asset_id_list[1:])
        ]
    subquery = first_subquery + " ".join(subqueries)
    complete_query = MAGNITUDE_PREFIX + subquery
    return complete_query

# COMMAND ----------

def handle_dss(
    asset_id_list: list[str],
    data_frequency_list: list[str],
    function: str,
    wire: str,
    wire_length_from: int,
    wire_length_to: int,
) -> str:
    """Handles query generation for 'dss'"""
    if len(asset_id_list) == 0 or asset_id_list is None or None in asset_id_list:
        return "asset_id_null"
    if function in ["average", "minimum", "maximum"]:
        function_subquery = generate_subquery(
            DSS_TEMPLATE_WITHOUT_CLASS_FUNCTION, asset_id_list[0], 1
        )
        inner_query = (
            function_subquery.replace("$function", DSS_FUNCTION_MAPPING[function])
            .replace("$length_from", str(wire_length_from))
            .replace("$length_to", str(wire_length_to))
            .replace("$wire", wire)
        )
    elif function in ["raw"]:
        function_subquery = generate_subquery(
            DSS_TEMPLATE_WITHOUT_CLASS_RAW, asset_id_list[0], 1
        )
        inner_query = (
            function_subquery.replace("$wire", wire)
            .replace("$length_from", str(wire_length_from))
            .replace("$length_to", str(wire_length_to))
        )
    complete_query = DSS_PREFIX + inner_query
    return complete_query

# COMMAND ----------

@udf(returnType=StringType())
def generate_query_udf(
    asset_id_list: list[str],
    parameter: str,
    clss: int,
    data_frequency_list: list[str],
    function: str,
    wire: str,
    wire_length_from: int,
    wire_length_to: int,
) -> str:
    """
    Generates a SQL query to be executed in the rule engine based on the given parameters.
    """
    try:
        if parameter in ["pressure", "temperature"]:
            return handle_pressure_temperature(asset_id_list, data_frequency_list)
        elif parameter == "no_of_events":
            return handle_no_of_events(asset_id_list, clss)
        elif parameter == "magnitude":
            return handle_magnitude(asset_id_list, clss)
        elif parameter in ["surface_flow_rate", "well_head_pressure"]:
            return handle_flow_meter(asset_id_list, data_frequency_list)
        # New logic for DSS
        elif parameter in ["dts", "axial_strain", "bend_magnitude"]:
            return handle_dss(
                asset_id_list,
                data_frequency_list,
                function,
                wire,
                wire_length_from,
                wire_length_to,
            )
        else:
            raise ValueError(f"Unsupported parameter: {parameter}")

    except Exception as e:
        raise

# COMMAND ----------

def get_rules_df(spark, logger, source_file_list):
    """
    This function reads the data from the json file list and returns a dataframe.
    """
    try:
        logger.info(f"Reading the source files : {source_file_list}")
        conditions_schema = StructType(
            [
                StructField("condition_id", IntegerType(), True),
                StructField("condition_name", StringType(), True),
                StructField("asset_id", ArrayType(StringType()), True),
                StructField("join_condition", StringType(), True),
                StructField("parameter", StringType(), True),
                StructField("operator", StringType(), True),
                StructField("class", IntegerType(), True),
                StructField("threshold", DoubleType(), True),
                StructField("duration", IntegerType(), True),
                StructField("wire", StringType(), True),
                StructField("function", StringType(), True),
                StructField("wire_length_from", IntegerType(), True),
                StructField("wire_length_to", IntegerType(), True),
                StructField("rule_run_frequency", IntegerType(), True),
                StructField("sensor_type", StringType(), True),
                StructField("severity", StringType(), True),
                StructField("risk_register_controls", ArrayType(IntegerType()), True),
            ]
        )

        schema = StructType(
            [
                StructField("rule_id", IntegerType()),
                StructField("rule_name", StringType(), True),
                StructField("tenant_id", StringType(), True),
                StructField("conditions", ArrayType(conditions_schema), True),
                StructField("operation", StringType(), True),
            ]
        )

        rules_df = read_json(spark, logger, source_file_list, schema).withColumn(
            "input_file", input_file_name()
        )

        return rules_df

    except Exception as e:
        logger.error(f"Error in get_rules_df() : {str(e)}")
        raise

# COMMAND ----------

def keep_latest_rules(logger, rules_df):
    """This function keeps only latest record for each rule id based on epoch timestamp extracted from file name."""
    try:
        rules_df = add_epoch_timestamp(logger, rules_df)
        window_spec = Window.partitionBy("rule_id").orderBy(desc("epoch_timestamp"))

        rules_df = (
            rules_df.withColumn("row_num", row_number().over(window_spec))
            .where(col("row_num") == 1)
            .drop("row_num", "epoch_timestamp")
        )

        return rules_df

    except Exception as e:
        logger.error(f"Error in keep_latest_rules() : {str(e)}")
        raise

# COMMAND ----------

def delete_rules(spark, logger, rules_df, bronze_table_name):
    """
    This function deletes the rules from the rules bronze table for records with operation as delete.
    """
    try:
        rules_delete_df = rules_df.filter(
            lower(trim(col("operation"))) == "delete"
        ).distinct()

        if rules_delete_df.count() > 0:
            logger.info("Found rules to Delete. Deleting the rules from delta table...")
            rules_dt = read_delta_table(spark, logger, bronze_table_name)

            (
                rules_dt.alias("target")
                .merge(
                    rules_delete_df.alias("source"), "target.rule_id = source.rule_id"
                )
                .whenMatchedDelete()
                .execute()
            )

        else:
            logger.info("No rules to delete...")

    except Exception as e:
        logger.error(f"Error in delete_rules() : {str(e)}")
        raise

# COMMAND ----------

def apply_join_condition(logger, rules_df):
    """
    This function applies the 'and' and 'or' join condition on the rules with operation as create/update and returns the resulting dataframe.
    """
    try:
        logger.info(f"Extracting the create/update rules...")
        create_update_rules_df = rules_df.filter(
            col("operation") != "delete"
        ).distinct()

        logger.info("Exploding conditions column and extracting values...")
        create_update_rules_df = (
            create_update_rules_df.withColumn("conditions", explode(col("conditions")))
            .withColumn("condition_id", col("conditions")["condition_id"])
            .withColumn("condition_name", col("conditions")["condition_name"])
            .withColumn("asset_id", col("conditions")["asset_id"])
            .withColumn("join_condition", col("conditions")["join_condition"])
            .withColumn("parameter", col("conditions")["parameter"])
            .withColumn("operator", col("conditions")["operator"])
            .withColumn("class", col("conditions")["class"])
            .withColumn("threshold", col("conditions")["threshold"])
            .withColumn("duration", col("conditions")["duration"])
            .withColumn("wire", col("conditions")["wire"])
            .withColumn("function", col("conditions")["function"])
            .withColumn("wire_length_from", col("conditions")["wire_length_from"])
            .withColumn("wire_length_to", col("conditions")["wire_length_to"])
            .withColumn("rule_run_frequency", col("conditions")["rule_run_frequency"])
            .withColumn("sensor_type", col("conditions")["sensor_type"])
            .withColumn("severity", col("conditions")["severity"])
            .withColumn(
                "risk_register_controls", col("conditions")["risk_register_controls"]
            )
            .withColumn("query", lit("query"))
            .withColumn("last_updated_date", lit(datetime.now()))
            .drop(col("conditions"))
        )
        if create_update_rules_df.filter(col("join_condition") != "").count() > 0:
            logger.info("Exploding asset_id column for join condition as 'or'...")
            create_update_rules_or_df = create_update_rules_df.filter(
                (lower(trim(col("join_condition"))) == "or")
            ).withColumn("asset_id", explode(col("asset_id")))
            logger.info("Concatenating asset_id column for join condition as 'and'...")
            create_update_rules_and_df = create_update_rules_df.filter(
                (lower(trim(col("join_condition"))) == "and")
            ).withColumn("asset_id", concat_ws(",", col("asset_id")))
            rules_df = create_update_rules_and_df.union(create_update_rules_or_df)

            logger.info(
                "Generating asset_id column as an array of string from string..."
            )
        else:
            logger.info("Generating df for non null join condition...")
            rules_df = create_update_rules_df.withColumn(
                "asset_id", concat_ws(",", col("asset_id"))
            )

        return rules_df.withColumn("asset_id", split("asset_id", ","))

    except Exception as e:
        logger.error(f"Error in apply_join_condition() : {str(e)}")
        raise

# COMMAND ----------

def get_rules_to_upsert(logger, rules_df, rules_dt):
    """
    This function returns a dataframe containing rules that needs to be created/inserted and updated.
    Note : It only retains rules with create operation which are not present in the rules bronze table.
    """
    try:
        rules_dt_df = rules_dt.toDF()

        logger.info(
            "Filtering rules where operation is 'create' and rule_id is not present in bronze table..."
        )

        create_src = rules_df.filter(lower(trim(col("operation"))) == "create")

        # A left_anti join returns all rows from the left DataFrame create_src that do not have a match in the right DataFrame rules_dt_df. In this case, it returns all rules in create_src whose rule_id does not exist in rules_dt_df.
        rules_create_df = create_src.join(
            rules_dt_df, create_src.rule_id == rules_dt_df.rule_id, how="left_anti"
        )
        logger.info("Filtering rules where operation is 'update'...")

        rules_update_df = rules_df.filter(lower(trim(col("operation"))) == "update")

        return rules_create_df.union(rules_update_df)

    except Exception as e:
        logger.error(f"Error in get_rules_to_upsert() : {str(e)}")
        raise

# COMMAND ----------

def add_data_frequency(logger, rules_df, asset_df):
    """
    This function adds data frequnecy column from asset bronze table in the rules dataframe.
    """
    try:
        logger.info("Fetching asset_id and data_frequency from asset bronze table...")
        asset_df = asset_df.select(col("asset_id"), col("data_frequency")).withColumn(
            "data_frequency", 2 * col("data_frequency") - 1
        )

        # Add a unique id to each row in rules_df. This is being used instead of using business key (asset_id, rule_id, condition_id) because for 'Or' it is exploded before
        logger.info("Adding an unique id in each row of rules...")
        rules_df = rules_df.withColumn("id", monotonically_increasing_id())

        # Explode the asset_id array in rules_df
        exploded_rules_df = rules_df.select(
            "id", explode(rules_df.asset_id).alias("asset_id")
        )

        logger.info("Joining rules and asset data based on asset_id...")
        joined_df = exploded_rules_df.join(asset_df, on="asset_id", how="left")

        # Group by id and collect data_frequency into a list
        logger.info("Collecting data frequency in a list...")
        result_df = joined_df.groupBy("id").agg(
            collect_list("data_frequency").alias("data_frequency"),
            ((max("data_frequency") + 1) / 2)
            .cast("integer")
            .alias("max_data_frequency"),
        )

        # Join back with rules_df to get the original asset_id array
        logger.info("Adding data_frequency column...")
        result_df = rules_df.join(result_df, on="id", how="inner")

        return result_df

    except Exception as e:
        logger.error(f"Error in add_data_frequency() : {str(e)}")
        raise

# COMMAND ----------

@udf(returnType=StringType())
def replace_parameter(query, parameter):
    if parameter in DSS_PARAMETER_MAPPING.keys():
        return query.replace("$parameter", DSS_PARAMETER_MAPPING[parameter])
    return query.replace("$parameter", parameter)

# COMMAND ----------

def add_rule_query(logger, rules_df):
    """
    This function generates rule SQL query as per the template and return two dataframes - one containing the generated rules and one containing the malformed generated rules.
    """
    try:
        # Generating Query based on asset id list. UDF accepts array of string where each element of the array would be utilized for query generation.
        logger.info("Generating rule query...")
        rules_df = rules_df.withColumn(
            "query",
            lit(
                generate_query_udf(
                    col("asset_id"),
                    col("parameter"),
                    col("class"),
                    col("data_frequency"),
                    col("function"),
                    col("wire"),
                    col("wire_length_from"),
                    col("wire_length_to"),
                )
            ),
        )
        logger.info("Filtering errorneous records...")
        rules_null_df = rules_df.filter(col("query") == "data_frequency_null")

        logger.info("Filtering records with generated query...")
        rules_df = rules_df.filter(col("query") != "data_frequency_null")

        logger.info("Replacing parameters in query with values...")

        rules_df = (
            rules_df.withColumn(
                "query", replace_parameter(col("query"), col("parameter"))
            )
            .withColumn("query", expr("replace(query,'$operator', operator)"))
            .withColumn(
                "query",
                when(
                    col("query").contains("$duration"),
                    expr("replace(query,'$duration', duration)"),
                ).otherwise(col("query")),
            )
            .withColumn("query", expr("replace(query,'$threshold', threshold)"))
            .withColumn(
                "query",
                when(
                    col("query").contains("$class"),
                    expr("replace(query,'$class', class)"),
                ).otherwise(col("query")),
            )
        )

        return rules_df, rules_null_df

    except Exception as e:
        logger.error(f"Error in add_rule_query() : {str(e)}")
        raise

# COMMAND ----------

def log_error_rules(
    logger,
    error_rules_df,
    bronze_error_log,
    job_id,
    run_id,
    task_id,
    workflow_name,
    task_name,
):
    """
    This function logs the malformed rules in the error log table
    """
    try:
        error_rules_df = (
            error_rules_df.select("asset_id", "input_file")
            .withColumn("workflow_job_id", lit(job_id))
            .withColumn("run_id", lit(run_id))
            .withColumn("task_id", lit(task_id))
            .withColumn("workflow_name", lit(workflow_name))
            .withColumn("task_name", lit(task_name))
            .withColumn("source", col("input_file"))
            .withColumn(
                "error_message",
                concat(
                    lit(
                        "Data Frequency missing in asset table for the asset_id/ some of the the asset_ids (in case of 'AND'). List of Asset id/s: "
                    ),
                    regexp_replace(concat_ws(", ", col("asset_id")), "^,\\s", ""),
                ),
            )
            .withColumn("additional_context", lit("NA"))
            .withColumn("last_updated_date", current_timestamp())
            .drop("asset_id", "input_file")
        )

        logger.info(f"Writing malformed rules into {bronze_error_log} table...")
        write_table(logger, error_rules_df, "append", bronze_error_log)

    except Exception as e:
        logger.error(f"Error in log_error_rules() : {str(e)}")
        raise

# COMMAND ----------

def delete_rules_with_update(logger, rules_df, rules_dt):
    """
    This function deletes rules from the bronze table where operation is 'update'.
    Note: Here, update is treated as delete and insert.
    """
    try:
        logger.info(
            "Deleting the rules from bronze table where operation == 'update' ..."
        )

        # Merge the rules_src_df with the bronze table rules_dt, deleting rows with matching rule_id where the operation is "update"
        # Mark the matched rows for deletion and insert the new rows
        (
            rules_dt.alias("tgt")
            .merge(
                rules_df.alias("src"),
                "src.rule_id = tgt.rule_id and src.operation = 'update'",
            )
            .whenMatchedDelete()
            .execute()
        )

    except Exception as e:
        logger.error(f"Error in delete_rules_with_update() : {str(e)}")
        raise

# COMMAND ----------

def write_rules(logger, rules_df, bronze_table_name):
    """
    This function writes the rule in the bronze table.
    """
    try:
        logger.info("Removing operation, id, data_frequency and input_file column...")
        rules_df = rules_df.drop(
            col("operation"), col("id"), col("data_frequency"), col("input_file")
        )

        logger.info("Rearranging columns to match the bronze table column sequence...")
        rules_df = rules_df.select(
            "rule_id",
            "rule_name",
            "tenant_id",
            "condition_id",
            "condition_name",
            "asset_id",
            "join_condition",
            "parameter",
            "operator",
            "class",
            "threshold",
            "duration",
            "wire",
            "function",
            "wire_length_from",
            "wire_length_to",
            "rule_run_frequency",
            "max_data_frequency",
            "sensor_type",
            "severity",
            "risk_register_controls",
            "query",
            "last_updated_date",
        )

        logger.info("Inserting the rules from source file...")
        write_table(logger, rules_df, "append", bronze_table_name)

    except Exception as e:
        logger.error(f"Error in write_rules() : {str(e)}")
        raise

# COMMAND ----------

def load_rule_bronze(
    spark,
    logger,
    source_file_list,
    bronze_table_name,
    bronze_asset_table,
    bronze_error_log,
    job_id,
    run_id,
    task_id,
    workflow_name,
    task_name,
) -> bool:
    try:
        rules_df = get_rules_df(spark, logger, source_file_list)

        if rules_df.count() == 0:
            logger.info("No rules to create/update/delete...")
            return False

        rules_df = keep_latest_rules(logger, rules_df)

        delete_rules(spark, logger, rules_df, bronze_table_name)

        rules_df = apply_join_condition(logger, rules_df)

        if rules_df.count() == 0:
            logger.info("No rules to create/update...")
            return True

        rules_dt = read_delta_table(spark, logger, bronze_table_name)

        rules_df = get_rules_to_upsert(logger, rules_df, rules_dt)

        asset_df = read_table(spark, logger, bronze_asset_table)

        rules_df = add_data_frequency(logger, rules_df, asset_df)

        rules_df, error_rules_df = add_rule_query(logger, rules_df)

        log_error_rules(
            logger,
            error_rules_df,
            bronze_error_log,
            job_id,
            run_id,
            task_id,
            workflow_name,
            task_name,
        )

        delete_rules_with_update(logger, rules_df, rules_dt)

        write_rules(logger, rules_df, bronze_table_name)

        return True

    except Exception as e:
        logger.error(f"Error in load_rule_bronze : {str(e)}")
        raise
