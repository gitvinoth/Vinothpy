# Databricks notebook source
from minio import Minio
from minio.commonconfig import CopySource
from pyspark.sql.functions import (
    flatten,
    collect_list,
    array,
    first,
    struct,
    col,
    udf,
    floor,
    array_min,
    split,
    explode,
    posexplode,
    avg,
    sort_array,
    size,
    array_sort,
)
import os
import zipfile
import tempfile
import json
from pyspark.sql import DataFrame
from delta import DeltaTable
from pyspark.sql.types import (
    ArrayType,
    DoubleType,
    IntegerType,
    FloatType,
    StringType,
    LongType,
    StructType,
    StructField,
)
from pyspark.sql.window import Window
from datetime import datetime, timedelta, time as t
from pyspark.sql import functions as F
import numpy as np
from pyspark.sql import types as T
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf

# COMMAND ----------

import json
import zipfile
import os
import time
import gzip
import bz2
import tarfile
import logging

# COMMAND ----------

try:
    if dbutils:
        pass
except NameError:
    from src.utils.read_utility import read_delta_table
    from src.utils.on_premise.utility_functions import copy_file

# COMMAND ----------

def das_data_filter(
    spark, logger, latest_data_time: int, time_window: int, silver_table_name: str
) -> DataFrame:
    """
    This function filters the data from the silver table based on latest_data_time and time window and returns the resultant dataframe.
    """
    try:
        logger.info("Reading the das delta table...")

        # Read the silver table
        silver_table_df = spark.read.table(silver_table_name)

        if time_window == 604800:
            start_datetime = datetime.fromtimestamp(latest_data_time)
            # Adjust start_time to the nearest 6-hour interval
            hour = start_datetime.hour
            adjusted_hour = (hour // 6) * 6
            start_datetime = start_datetime.replace(
                hour=adjusted_hour, minute=0, second=0, microsecond=0
            )
            start_time = int(start_datetime.timestamp())

            end_time = start_time + 21600 - 1  # 6 hours in seconds

            logger.info(
                f"Filtering data with start_time: {start_time} and end_time: {end_time}"
            )

            # Filter rows based on time range
            filtered_df = silver_table_df.filter(
                (F.col("time") >= start_time) & (F.col("time") <= end_time)
            )
        elif time_window == 2592000:
            # Convert start_time to datetime
            start_datetime = datetime.fromtimestamp(latest_data_time)

            # Adjust start_time to 12 AM and end_time to 11:59:59 PM of the same day
            start_datetime = start_datetime.replace(
                hour=0, minute=0, second=0, microsecond=0
            )
            end_datetime = start_datetime.replace(
                hour=23, minute=59, second=59, microsecond=59
            )
            start_time = int(start_datetime.timestamp())
            end_time = int(end_datetime.timestamp())

            logger.info(
                f"Filtering data with start_time: {start_time} and end_time: {end_time}"
            )

            # Filter rows based on time range
            filtered_df = silver_table_df.filter(
                (F.col("time") >= start_time) & (F.col("time") <= end_time)
            )
        else:
            seed_time = latest_data_time % time_window
            start_time = latest_data_time - seed_time
            end_time = start_time + time_window - 1

            logger.info(
                f"Filtering data with start_time: {start_time} and end_time: {end_time}"
            )

            # Filter rows based on time range
            filtered_df = silver_table_df.filter(
                (F.col("time") >= start_time) & (F.col("time") <= end_time)
            )
        return filtered_df

    except Exception as e:
        if "TableNotFoundException" in str(e):
            logger.error(f"Error in das_data_filter | Table not found: {str(e)}")
            raise

        else:
            logger.error(f"Error in das_data_filter : {str(e)}")
            raise

# COMMAND ----------

def das_data_window_append(
    logger, das_df_filtered: DataFrame, sample_size: int
) -> DataFrame:
    """
    This function groups the data on "Frequencyband", "device_id" and appends all the records within the same "Frequencyband", "device_id". After that, it creates an additional column called json_output where it puts the data , time, and depth column in struct format. This column would then be used for publishing the files.
    """
    try:
        # Add a bucket column to divide data into chunks with a maximum sample size
        df_bucketed = das_df_filtered.withColumn(
            "bucket",
            F.ntile(sample_size).over(
                Window.partitionBy("freq_band", "asset_id").orderBy("time")
            ),
        )

        # Pick the **first** time for each bucket interval
        df_time_buckets = df_bucketed.withColumn(
            "first_time",
            F.first("time").over(
                Window.partitionBy("freq_band", "asset_id", "bucket").orderBy("time")
            ),
        )

        # Explode the data array and the time for each bucket to match the time intervals
        df_exploded = df_time_buckets.select(
            "*", F.posexplode("data").alias("pos", "value")
        )

        # Calculate the average for each bucket and pos for data. Remove Device_id
        df_avg_bucketed = df_exploded.groupBy(
            "freq_band", "asset_id", "device_id", "bucket", "pos"
        ).agg(F.avg("value").alias("avg_value"))

        # Pivot to transpose
        df_transposed = (
            df_avg_bucketed.groupBy("freq_band", "asset_id", "device_id", "pos")
            .pivot("bucket")
            .agg(F.first("avg_value"))
        )

        int_column_names = sorted(
            [
                int(col)
                for col in df_transposed.columns
                if col not in ["freq_band", "asset_id", "device_id", "pos"]
            ]
        )

        # Once the columns are converted we are converting them to string. e.g. ['1','2','3','4',...]
        column_names = [str(col) for col in int_column_names]

        df_transposed = df_transposed.orderBy("pos")

        # Collect the transposed data into a single column of 2D arrays
        df_transposed = df_transposed.groupBy("freq_band", "asset_id", "device_id").agg(
            F.collect_list(F.array(column_names)).alias("data")
        )

        # Now get the first time for each bucket (we already calculated it earlier)
        df_time_result = df_time_buckets.groupBy(
            "freq_band", "asset_id", "device_id", "bucket"
        ).agg(
            F.first("first_time").alias("time")
        )  # Ensure we pick the first time for the bucket

        df_time_result = df_time_result.groupBy(
            "freq_band", "asset_id", "device_id"
        ).agg(F.collect_list("time").alias("time"))

        # Join the average data and the first time value for each bucket
        df_combined = df_transposed.join(
            df_time_result, on=["freq_band", "asset_id", "device_id"]
        )

        # Group by freq_band, asset_id, and device_id to collect all data and time into 2D lists across buckets
        df_final = df_combined.groupBy("freq_band", "asset_id", "device_id").agg(
            F.collect_list("data").alias("data"), F.collect_list("time").alias("time")
        )

        # Collect depth (assuming it is constant within each group) and add it back to the final DataFrame
        df_depth = das_df_filtered.groupBy("freq_band", "asset_id", "device_id").agg(
            F.first("depth").alias("depth")
        )

        df_final_with_depth = df_final.join(
            df_depth, on=["freq_band", "asset_id", "device_id"]
        )

        df_final_with_depth = df_final_with_depth.withColumn(
            "data", F.flatten(F.col("data"))
        ).withColumn("time", F.flatten(F.col("time")))

        # Create a JSON structure for data, time, and depth columns
        das_df_json = df_final_with_depth.withColumn(
            "json_output", F.struct(F.col("data"), F.col("time"), F.col("depth"))
        )

        # Select the required columns to display
        das_df_json = das_df_json.select(
            "freq_band", "asset_id", "device_id", "json_output"
        )

        return das_df_json

    except Exception as e:
        if "AnalysisException" in str(e):
            logger.error(f"Error in das_data_window_append: {str(e)}")
            raise

        else:
            logger.error(f"Error in das_data_window_append : {str(e)}")
            raise

# COMMAND ----------

#def list_directory(path: str):
#    """
#    List the contents of a directory.
#    """
#    try:
#        return [file.name for file in dbutils.fs.ls(path)]
#    except Exception as e:
#        if "FileNotFoundException" in str(e):
#            return []
#        else:
#            raise

# COMMAND ----------
try:
    dbutils
    # Use dbutils-based versions
except NameError:
    # Fallback to Jupyter-compatible versions
    import os, shutil
    def list_directory(path: str):
        try:
            return os.listdir(path)
        except FileNotFoundError:
            return []
        except Exception as e:
            raise


            
def delete_previous_day_data(spark, logger, silver_table_name: str, latest_data_time: int):
    """
    This function deletes the previous day's data from the silver table if run during the first 30 min of the day
    """
    try:
        now = datetime.fromtimestamp(latest_data_time)
        start_of_today = now.replace(
            hour=0, minute=0, second=0, microsecond=0
        )

        start_of_today_epoch = int(start_of_today.timestamp())

        logger.info(f"Deleting data from silver table '{silver_table_name}' where time < {start_of_today_epoch}")
                    
        spark.sql(f"DELETE FROM {silver_table_name} WHERE time < {start_of_today_epoch}")
        logger.info("Previous day's data deleted successfully")

    except Exception as e:
        logger.error(f"Error in delete_previous_day_data: {str(e)}")
        raise

# COMMAND ----------

def is_end_of_6th_hour_window(current_time: datetime) -> bool:
    """
    This function checks if the current running time is at the end of a 6-hour window.
    """
    current_time = datetime.fromtimestamp(current_time)
    return current_time.hour % 6 == 5 and current_time.minute > 30

# COMMAND ----------

def is_end_of_day(current_time: datetime) -> bool:
    """
    This function checks if the current running time is at the end of the day.
    """
    current_time = datetime.fromtimestamp(current_time)
    return current_time.hour == 23 and current_time.minute > 30

# COMMAND ----------

def get_max_time_of_data(spark, silver_table_name: str) -> datetime:
    """
    This function gets the maximum time of the current day's data from the silver table.
    """
    try:
        # Read the silver table
        silver_table_df = spark.read.table(silver_table_name)

        # Get the maximum time from the filtered data
        max_time_epoch = silver_table_df.agg(F.max("time")).collect()[0][0]

        return datetime.fromtimestamp(max_time_epoch)

    except Exception as e:
        if "TableNotFoundException" in str(e):
            logger.error(
                f"Error in get_max_time_of_current_day | Table not found: {str(e)}"
            )
            raise
        else:
            logger.error(f"Error in get_max_time_of_current_day : {str(e)}")
            raise


            
# COMMAND ----------

from urllib.parse import urlparse
def copy_file(source: str, dest: str):
   try:
       # Remove 'file:' prefix if present
       if source.startswith("file:"):
           source = source.replace("file:", "")
       if not os.path.exists(source):
           raise FileNotFoundError(f"Source file not found: {source}")
       # Extract bucket and object key from dest
       # Example dest: 'ccus-dd40badc-44a8-4c24-994a-ab80edc83478/gold_zone/published/...'
       bucket, *key_parts = dest.strip("/").split("/", 1)
       object_name = key_parts[0] if key_parts else ""
       # Initialize MinIO client
       minio_client = Minio(
           os.environ["s3_endpoint"],
           access_key=os.environ["s3_accesskey"],
           secret_key=os.environ["s3_secret"],
           secure=False,
       )
       # Upload file
       minio_client.fput_object(bucket, object_name, source)
       print(f" Uploaded {source} to minio://{bucket}/{object_name}")
   except Exception as e:
       print(f" Failed to upload file to MinIO: {str(e)}")
       raise

# COMMAND ----------

def write_and_zip_json_data(
   logger,
   json_data,
   asset_id,
   device_id,
   freq_bd_val,
   epoch_start_time,
   freq,
   cloud_zip_file_path,
):
   try:
       # Sanitize asset_id
       safe_asset_id = asset_id.replace("/", "_").replace(" ", "_")
       # MinIO client setup
       bucket = os.environ["storage_host"]
       minio_client = Minio(
           os.environ["s3_endpoint"],
           access_key=os.environ["s3_accesskey"],
           secret_key=os.environ["s3_secret"],
           secure=False,
       )
       with tempfile.TemporaryDirectory() as temp_dir:
           json_filename = f"{safe_asset_id}_{freq_bd_val}_{epoch_start_time}_{freq}.json"
           zip_filename = json_filename.replace(".json", ".zip")
           local_json_path = os.path.join(temp_dir, json_filename)
           local_zip_path = os.path.join(temp_dir, zip_filename)
           # Write JSON file
           with open(local_json_path, "w") as f:
               f.write(json_data)
           # Create ZIP file
           with zipfile.ZipFile(local_zip_path, "w", compression=zipfile.ZIP_DEFLATED, compresslevel=1) as zipf:
               zipf.write(local_json_path, arcname=json_filename)
           # Upload ZIP to MinIO temp folder
           temp_object_path = f"gold_zone/temp/{safe_asset_id}/{zip_filename}"
           minio_client.fput_object(bucket, temp_object_path, local_zip_path)
           logger.info(f" Temp ZIP uploaded to minio://{bucket}/{temp_object_path}")
           # Copy ZIP from temp path to final cloud path
           _, cloud_object_key = cloud_zip_file_path.split("/", 1)
           source = CopySource(bucket_name=bucket, object_name=temp_object_path)
           minio_client.copy_object(bucket_name=bucket, object_name=cloud_object_key, source=source)
           logger.info(f" Final ZIP copied to minio://{bucket}/{cloud_object_key}")
           logger.info(f" Temp retained at minio://{bucket}/{temp_object_path}")
   except Exception as e:
       logger.error(f"âŒ Error in write_and_zip_json_data (MinIO): {str(e)}")
       raise
       
# COMMAND ----------

def append_and_zip_json_data(
   logger,
   cloud_zip_file_path,
   json_data,
   asset_id,
   freq_bd_val,
   epoch_start_time,
   freq,
):
   try:
       # MinIO client setup
       bucket = os.environ["storage_host"]
       minio_client = Minio(
           os.environ["s3_endpoint"],
           access_key=os.environ["s3_accesskey"],
           secret_key=os.environ["s3_secret"],
           secure=False,
       )
       # Sanitize asset_id
       safe_asset_id = asset_id.replace("/", "_").replace(" ", "_")
       zip_filename = f"{safe_asset_id}_{freq_bd_val}_{epoch_start_time}_{freq}.zip"
       json_filename = zip_filename.replace(".zip", ".json")
       with tempfile.TemporaryDirectory() as temp_dir:
           local_zip_path = os.path.join(temp_dir, zip_filename)
           local_json_path = os.path.join(temp_dir, json_filename)
           # Download current cloud ZIP
           _, cloud_object_key = cloud_zip_file_path.split("/", 1)
           minio_client.fget_object(bucket, cloud_object_key, local_zip_path)
           # Extract JSON
           with zipfile.ZipFile(local_zip_path, "r") as zipf:
               zipf.extract(json_filename, temp_dir)
           with open(local_json_path, "r") as f:
               existing_data = json.load(f)
           new_data = json.loads(json_data)
           existing_data["data"] = [x + y for x, y in zip(existing_data["data"], new_data["data"])]
           existing_data["time"].extend(new_data.get("time", []))
           with open(local_json_path, "w") as f:
               json.dump(existing_data, f)
           # Re-zip updated JSON
           updated_zip_path = os.path.join(temp_dir, f"updated_{zip_filename}")
           with zipfile.ZipFile(updated_zip_path, "w", compression=zipfile.ZIP_DEFLATED, compresslevel=1) as zipf:
               zipf.write(local_json_path, arcname=json_filename)
           # Upload to MinIO temp folder
           temp_object_path = f"gold_zone/temp/{safe_asset_id}/{zip_filename}"
           minio_client.fput_object(bucket, temp_object_path, updated_zip_path)
           # Copy to final cloud path
           source = CopySource(bucket_name=bucket, object_name=temp_object_path)
           minio_client.copy_object(bucket_name=bucket, object_name=cloud_object_key, source=source)
           logger.info(f" Appended and published to minio://{bucket}/{cloud_object_key}")
           logger.info(f" Temp ZIP retained at minio://{bucket}/{temp_object_path}")
   except Exception as e:
       logger.error(f" Error in append_and_zip_json_data (MinIO): {str(e)}")
       raise
       
# COMMAND ----------

#def copy_file(source: str, dest: str):
#    try:
#        dbutils.fs.cp(source, dest)
#    except Exception as e:
#        raise


def das_file_publish(
    logger,
    input_df: DataFrame,
    target_file_path: str,
    epoch_start_time: int,
    time_window: int,
):
    """
    This function generates each file, zips the file, and publishes the file
    """
    try:
        if input_df.count() == 0:
            logger.info("Input DataFrame is empty. No data to publish.")
            return

        if time_window == 1800:
            freq = "30mins"
        elif time_window == 3600:
            freq = "60mins"
        elif time_window == 86400:
            freq = "1day"
        elif time_window == 604800:
            freq = "1week"
        elif time_window == 2592000:
            freq = "1month"

        # Function to convert row to JSON
        def row_to_json(row):
            return json.dumps(row.asDict(), ensure_ascii=False)

        # Iterate over each row in the DataFrame
        for row in input_df.collect():
            # Get the fr_bd value and json_output from the row
            freq_bd_val = row.freq_band
            json_output = row.json_output
            device_id = row.device_id
            asset_id = row.asset_id

            # Construct the path where files would be published
            write_file_path = f"{target_file_path}/{asset_id}"

            # Convert the json_output to JSON format
            json_data = row_to_json(json_output)

            # Construct the json file path
            file_path = (
                f"{write_file_path}/{freq_bd_val}/{epoch_start_time}_{freq}.json"
            )

            # Construct the cloud file path with the final file name
            cloud_zip_file_path = (
                f"{write_file_path}/{freq_bd_val}/{epoch_start_time}_{freq}.zip"
            )

            if time_window in [604800, 2592000]:
                # Check if the cloud zip file path already exists
                file_exists = False
                try:
                    if cloud_zip_file_path.split("/")[-1] in list_directory(
                        f"{write_file_path}/{freq_bd_val}"
                    ):
                        file_exists = True
                except Exception as e:
                    if "FileNotFoundException" in str(e):
                        file_exists = False
                    else:
                        raise

                if file_exists:
                    append_and_zip_json_data(
                        logger,
                        cloud_zip_file_path,
                        json_data,
                        asset_id,
                        freq_bd_val,
                        epoch_start_time,
                        freq,
                    )
                else:
                    # Construct the local file path where files will be written for zipping files locally
                    write_and_zip_json_data(
                        logger,
                        json_data,
                        asset_id,
                        device_id,
                        freq_bd_val,
                        epoch_start_time,
                        freq,
                        cloud_zip_file_path,
                    )
            else:
                # Directly create the file in the cloud for other time windows

                write_and_zip_json_data(
                    logger,
                    json_data,
                    asset_id,
                    device_id,
                    freq_bd_val,
                    epoch_start_time,
                    freq,
                    cloud_zip_file_path,
                )

    except Exception as e:
        if "OSError" in str(e):
            logger.error(f"Error in das_file_publish: {str(e)}")
            raise

        else:
            logger.error(f"Error in das_file_publish : {str(e)}")
            raise

# COMMAND ----------

def current_window_start_time_das(
    spark, logger, time_window: int, latest_data_time: int
) -> int:
    try:
        start_time_date = datetime.fromtimestamp(latest_data_time)
        start_time = latest_data_time


        if time_window in [1800, 3600, 86400]:
            seed_time = latest_data_time % time_window
            start_time = latest_data_time - seed_time
        elif time_window == 604800:
            start_of_this_week = start_time_date - timedelta(
                days=start_time_date.weekday()
            )  # Start of this week (Monday)
            start_of_this_week = (start_of_this_week).replace(
                hour=0, minute=0, second=0
            )
            start_time = int(start_of_this_week.timestamp())
        elif time_window == 2592000:
            # Get the start of the month
            month_start = start_time_date.replace(
                day=1, hour=0, minute=0, second=0
            ).timestamp()
            start_time = int(month_start)
        if start_time == 0:
            raise ValueError("Invalid time window provided")

        logger.info(f"Current window start time is: {start_time}")
        return start_time
    
    except Exception as e:
        if "ZeroDivisionError" in str(e):
            logger.error(f"Error in current_window_start_time: {str(e)}")
            raise
        else:
            logger.error(f"Error in current_window_start_time : {str(e)}")
            raise

# COMMAND ----------

# Function to truncate data based on the epoch time column ('time')
def truncate_data(spark, logger, silver_table) -> None:
    """
    Delete previous month data from the silver table on the first monday of the current month.
    """
    try:
        logger.info("Starting data truncation process...")

        # Get the current date
        current_date = datetime.now()
        day = current_date.day
        day_of_week = current_date.weekday()
        is_within_time_window = t(0, 0) <= current_date.time() < t(0, 30)

        # Check if today is the first Monday of the current month
        if day_of_week == 0 and day <= 7 and is_within_time_window:
            logger.info(
                "Today is the first Monday of the month. Proceeding with truncation..."
            )
            first_day_of_month = current_date.replace(
                day=1, hour=0, minute=0, second=0, microsecond=0
            )
            # Get the epoch time for the first of the last month
            epoch_time = int(first_day_of_month.timestamp())

            # Perform the DELETE operation to remove rows older than the first of last month
            silver_table.delete(F.col("time") < epoch_time)

        else:
            logger.info(
                "Today is not the first Monday of the month. Skipping truncation."
            )

    except Exception as e:
        logger.error(f"Error in truncate_data: {str(e)}")
        raise

# COMMAND ----------

def last_window_start_time_das(
    spark, logger, current_time_epoch: int, time_window: int
) -> int:
    try:
        logger.info("Inside last_window_start_time function")
        start_time = 0
        evaluation_time = current_time_epoch - time_window
        # Converting to date format
        dt = datetime.fromtimestamp(current_time_epoch)
        start_time_date = datetime.fromtimestamp(current_time_epoch).date()
        if time_window in [1800, 3600]:
            seed_time = evaluation_time % time_window
            start_time = evaluation_time - seed_time
        elif time_window == 86400:
            current_datetime = datetime.now()
            is_within_time_window = t(0, 0) <= current_datetime.time() < t(0, 30)
            if is_within_time_window:
                previous_day = current_datetime.date() - timedelta(days=1)
                start_time = int(datetime.combine(previous_day, t(0, 0)).timestamp())
        elif time_window == 604800:
            current_datetime = datetime.now()
            is_monday = current_datetime.weekday() == 0
            is_within_time_window = t(0, 0) <= current_datetime.time() < t(0, 30)
            logger.info(
                f"Time window : {time_window} | is_monday : {is_monday} | is_within_time_window : {is_within_time_window}"
            )
            if is_monday and is_within_time_window:
                previous_week_start = current_datetime - timedelta(days=7)
                start_time = int(
                    previous_week_start.replace(
                        hour=0, minute=0, second=0, microsecond=0
                    ).timestamp()
                )
        elif time_window == 2592000:
            current_datetime = datetime.now()
            is_first_day_of_month = current_datetime.day == 1
            is_within_time_window = t(0, 0) <= current_datetime.time() < t(0, 30)
            logger.info(
                f"Time window : {time_window} | is_first_day_of_month : {is_first_day_of_month} | is_within_time_window : {is_within_time_window}"
            )
            if is_first_day_of_month and is_within_time_window:
                logger.info("Inside if")
                first_day_of_current_month = current_datetime.replace(
                    day=1, hour=0, minute=0, second=0, microsecond=0
                )
                last_day_of_previous_month = first_day_of_current_month - timedelta(
                    days=1
                )
                start_time = int(last_day_of_previous_month.replace(day=1).timestamp())

        return start_time

    except Exception as e:
        if "ZeroDivisionError" in str(e):
            logger.error(f"Error in last_window_start_time: {str(e)}")
            raise
        else:
            logger.error(f"Error in last_window_start_time : {str(e)}")
            raise

# COMMAND ----------

def get_cw_das_df_filtered(
    spark,
    logger,
    latest_data_time: int,
    time_window: int,
    silver_table_name: str,
    sample_size: int,
):
    # Filter the data for the current window
    cw_das_df_filtered = das_data_filter(
        spark, logger, latest_data_time, time_window, silver_table_name
    )
    cw_df_append = das_data_window_append(logger, cw_das_df_filtered, sample_size)
    logger.info(
        f"Data found, current window execution will happen and total {cw_df_append.count()} files will be published"
    )
    return cw_df_append

# COMMAND ----------

def das_data_publish(
    spark,
    logger,
    silver_table_name: str,
    target_file_path: str,
    initation_time: int,
    time_window: int,
    sample_size: int,
    engine_run_frequency: int,
):
    """
    This function is the main function which controls all the logic by calling other functions. First it calculates start time and using that it filters necessary data from silver table. After that, it appends necessary data by groups and creates the necessary struct format data. Finally, it generates the zip file and publishes it.
    """
    try:
        # Read the silver table into a DataFrame
        silver_table_dt = read_delta_table(spark, logger, silver_table_name)
        silver_table_df = silver_table_dt.toDF()

        logger.info(
            f"Starting execution for execution time : {initation_time} and time_window : {time_window}"
        )
        # Get the latest data time
        latest_data_time = get_max_time_of_data(spark, silver_table_name)

        if latest_data_time is None:
            logger.info("No data available for the current date.")
            return
        # Ensure latest_data_time is a Unix timestamp
        if isinstance(latest_data_time, datetime):
            latest_data_time = int(latest_data_time.timestamp())

        if latest_data_time is not None:
            # Run always for the previous run cycle to accomodate all data received
            latest_data_time = latest_data_time - engine_run_frequency
            # Run weekly task only if latest data received has crossed 6 hour window
            
            if time_window == 604800 and is_end_of_6th_hour_window(
                latest_data_time
            ):
                logger.info(
                    f"Publishing the file at the end of the 6th-hour window for Weekly Publish task and the end_of_6th_hour_window is : {latest_data_time} and time_window is: {time_window}")
                
                # Calculate the window time as for file name
                cw_start_time = current_window_start_time_das(
                    spark,
                    logger,
                    time_window,
                    latest_data_time,
                )

                # Get filtered and summarized df
                cw_df_append = get_cw_das_df_filtered(
                    spark,
                    logger,
                    latest_data_time,
                    time_window,
                    silver_table_name,
                    sample_size,
                )

                # Publish the data
                das_file_publish(
                    logger,
                    cw_df_append,
                    target_file_path,
                    cw_start_time,
                    time_window,
                )
            elif time_window == 2592000 and is_end_of_day(latest_data_time):

                logger.info(f"Publishing the file at the end of the day for Monthly publish task and end_of_day_time is: {latest_data_time} and time_window is: {time_window}")

                # Calculate the window time as for file name
                cw_start_time = current_window_start_time_das(
                    spark,
                    logger,
                    time_window,
                    latest_data_time,
                )

                # Get filtered and summarized df
                cw_df_append = get_cw_das_df_filtered(
                    spark,
                    logger,
                    latest_data_time,
                    time_window,
                    silver_table_name,
                    sample_size,
                )
                # Publish the data
                das_file_publish(
                    logger,
                    cw_df_append,
                    target_file_path,
                    cw_start_time,
                    time_window,
                )

                # Delete the previous day's data from the silver table
                delete_previous_day_data(spark, logger, silver_table_name, latest_data_time)

            elif time_window in [1800, 3600, 86400]:

                logger.info(f"Publishing the file for 30min, 1hr, 1day data for the latest_Date_time:{latest_data_time} and current running time_widow is :{time_window}")

                # Calculate the window time as for file name
                cw_start_time = current_window_start_time_das(
                    spark,
                    logger,
                    time_window,
                    latest_data_time,
                )

                # Get filtered and summarized df
                cw_df_append = get_cw_das_df_filtered(
                    spark,
                    logger,
                    latest_data_time,
                    time_window,
                    silver_table_name,
                    sample_size,
                )
                # Publish the data
                das_file_publish(
                    logger,
                    cw_df_append,
                    target_file_path,
                    cw_start_time,
                    time_window,
                )
            else:
                logger.info(f"No file publish for the current_time:{latest_data_time}and it is not the end of {time_window}")
            
        else:
            logger.info(f"No file publish for the current_time:{latest_data_time}")

    except Exception as e:
        if "FileNotFoundError" in str(e):
            logger.error(f"Error in das_data_publish: {str(e)}")
            raise

        else:
            logger.error(f"Error in das_data_publish : {str(e)}")
            raise
