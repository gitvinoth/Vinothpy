from concurrent.futures import ThreadPoolExecutor, as_completed
from pyspark.sql import functions as F
from pyspark.sql.window import Window
import zipfile, os, json
def dts_file_publish(
   spark,
   logger,
   df: DataFrame,
   target_file_path: str,
   epoch_start_time: int,
   time_window: int,
   sample_size: int
):
   """
   ONE file per (asset_id, window) in depth-major multi-time shape with STRICTLY DISTINCT time:
     {
       "data":  [[d1_t1, d1_t2, ...], [d2_t1, d2_t2, ...], ...],
       "time":  [t1, t2, ...],      # distinct, strictly increasing
       "depth": [d1, d2, ...]       # numeric-looking labels as strings, ascending
     }
   Fast path (no pivot). All joins aliased. Time dedup is enforced via DISTINCT + row_number indexing.
   """
   try:
       # ----- frequency label -----
       if time_window == 1800:   freq = "30mins"
       elif time_window == 3600: freq = "60mins"
       elif time_window == 86400: freq = "1day"
       elif time_window == 604800: freq = "1week"
       elif time_window == 2592000: freq = "1month"
       else: raise ValueError(f"Unsupported time_window: {time_window}")
       # Validate columns
       required_cols = {"asset_id", "fiber_length", "epoch_timestamp", "temperature"}
       missing = required_cols - set(df.columns)
       if missing:
           raise ValueError(f"Input DataFrame missing columns: {missing}")
       # 1) Bucket rows per asset in time order
       w_order = Window.partitionBy("asset_id").orderBy(F.col("epoch_timestamp").asc())
       with_bkt = df.withColumn("bucket", F.ntile(sample_size).over(w_order))
       # 2) Aggregate per (asset, bucket, depth) -> representative time + avg value
       per_bkt_depth = (
           with_bkt
           .groupBy("asset_id", "bucket", "fiber_length")
           .agg(
               F.min("epoch_timestamp").alias("first_ts_in_bucket"),
               F.avg("temperature").alias("avg_temp")
           )
       )
       # 3) Collapse to (asset, depth, ts) (avg if multiple buckets map to same ts)
       per_time_depth = (
           per_bkt_depth
           .select(
               F.col("asset_id").alias("asset_id"),
               F.col("fiber_length").cast("string").alias("depth"),
               F.col("first_ts_in_bucket").cast("long").alias("ts"),
               F.col("avg_temp").alias("val")
           )
           .groupBy("asset_id", "depth", "ts")
           .agg(F.avg("val").alias("val"))
       )
       # 4) Build STRICT, DISTINCT, SORTED time per asset via explicit dedup + indexing
       #    - First distinct (asset_id, ts)
       #    - Then row_number() over order by ts to produce idx 0..N-1
       distinct_times = (
           per_time_depth
           .select("asset_id", "ts")
           .distinct()
       )
       w_time_idx = Window.partitionBy("asset_id").orderBy(F.col("ts").asc())
       times_with_idx = (
           distinct_times
           .withColumn("idx", F.row_number().over(w_time_idx) - F.lit(1))
           .select(
               F.col("asset_id").alias("t_asset_id"),
               F.col("ts").alias("t_ts"),
               F.col("idx").alias("t_idx")
           )
       )
       # 5) Join values to the time index, then build aligned series per (asset, depth)
       a_alias = per_time_depth.alias("a")
       ti_alias = times_with_idx.alias("ti")
       aligned = (
           a_alias.join(
               ti_alias,
               (F.col("a.asset_id") == F.col("ti.t_asset_id")) &
               (F.col("a.ts") == F.col("ti.t_ts")),
               "inner"
           )
           .select(
               F.col("a.asset_id").alias("asset_id"),
               F.col("a.depth").alias("depth"),
               F.col("ti.t_idx").alias("idx"),
               F.col("a.val").alias("val")
           )
       )
       depth_series = (
           aligned
           .groupBy("asset_id", "depth")
           .agg(F.sort_array(F.collect_list(F.struct("idx", "val"))).alias("pairs"))
           .select(
               "asset_id",
               "depth",
               F.transform(F.col("pairs"), lambda p: p["val"]).alias("series")
           )
       )
       # 6) Assemble per-asset depth list (sorted numeric) and data (depth-major)
       ds_alias = depth_series.alias("ds")
       by_asset = (
           ds_alias
           .groupBy("asset_id")
           .agg(F.collect_list(F.struct("depth", "series")).alias("pairs"))
           .select(
               "asset_id",
               F.expr("""
                   transform(
                     array_sort(
                       transform(pairs, p -> struct(try_cast(p.depth as double) as k, p.depth as d, p.series as s))
                     ),
                     x -> struct(x.d as depth, x.s as series)
                   )
               """).alias("sorted_pairs")
           )
           .select(
               "asset_id",
               F.transform(F.col("sorted_pairs"), lambda p: p["depth"]).alias("depth"),
               F.transform(F.col("sorted_pairs"), lambda p: p["series"]).alias("data")
           )
       )
       # 7) Collect the final DISTINCT time array (ordered) per asset from the indexed table
       #    Using the same index ensures the 'time' order matches the 'series' idx.
       times_ordered = (
           times_with_idx
           .groupBy("t_asset_id")
           .agg(
               F.collect_set("t_ts").alias("distinct_time_set")
           )
           .select(
               F.col("t_asset_id").alias("asset_id"),
               F.array_sort(F.col("distinct_time_set")).alias("time")
           )
       )
       # 8) Join back time; serialize JSON in Spark
       ba_alias = by_asset.alias("ba")
       to_alias = times_ordered.alias("to")
       per_asset_json_df = (
           ba_alias.join(to_alias, F.col("ba.asset_id") == F.col("to.asset_id"), "left")
                   .select(
                       F.col("ba.asset_id").alias("asset_id"),
                       F.coalesce(F.col("to.time"),  F.array().cast("array<long>")).alias("time"),
                       F.coalesce(F.col("ba.depth"), F.array().cast("array<string>")).alias("depth"),
                       F.coalesce(F.col("ba.data"),  F.array().cast("array<array<double>>")).alias("data")
                   )
                   .select(
                       "asset_id",
                       F.to_json(F.struct(F.col("data"), F.col("time"), F.col("depth"))).alias("json_str")
                   )
       )
       # 9) Collect tiny (asset_id, json) rows to driver
       small_rows = per_asset_json_df.collect()
       if not small_rows:
           logger.info("No assets to publish after aggregation.")
           return
       # 10) Upload per-asset files in parallel
       def publish_one(asset_id: str, json_str: str) -> str:
           write_dir  = f"{target_file_path}/{asset_id}"
           file_stem  = f"{asset_id}_{epoch_start_time}_{freq}"
           local_json = f"/tmp/{file_stem}.json"
           local_zip  = f"/tmp/{file_stem}.zip"
           cloud_zip  = f"{write_dir}/{epoch_start_time}_{freq}.zip"
           try:
               with open(local_json, "w") as f:
                   f.write(json_str)
               with zipfile.ZipFile(local_zip, "w", compression=zipfile.ZIP_DEFLATED) as zipf:
                   zipf.write(local_json, arcname=f"{file_stem}.json")
               dbutils.fs.cp(f"file:{local_zip}", cloud_zip, True)
               return f"[{asset_id}] Uploaded: {cloud_zip}"
           except Exception as e:
               return f"[{asset_id}] FAILED: {e}"
           finally:
               for p in (local_json, local_zip):
                   try:
                       os.remove(p)
                   except Exception:
                       pass
       with ThreadPoolExecutor(max_workers=30) as ex:
           futures = [ex.submit(publish_one, r["asset_id"], r["json_str"]) for r in small_rows]
           for fut in as_completed(futures):
               logger.info(fut.result())
   except Exception as e:
       logger.error(f"Error in dts_file_publish : {str(e)}")
       raise
