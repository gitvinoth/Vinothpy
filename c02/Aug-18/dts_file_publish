from concurrent.futures import ThreadPoolExecutor, as_completed
def dts_file_publish(
   spark,
   logger,
   df: DataFrame,
   target_file_path: str,
   epoch_start_time: int,
   time_window: int,
   sample_size: int
):
   """
   Publish ONE file per (asset_id, window) with depth-major multi-time JSON:
     {
       "data":  [[d1_t1, d1_t2, ...], [d2_t1, d2_t2, ...], ...],
       "time":  [t1, t2, ...],      # ordered by bucket ascending
       "depth": [d1, d2, ...]       # numeric-looking depth labels as strings, ascending
     }
   Steps:
     1) Pivot fiber_length -> depth columns (once).
     2) Bucket rows per asset into 'sample_size' tiles by epoch_timestamp.
     3) AVG each depth column within bucket; MIN(epoch_timestamp) as the bucket time.
     4) Aggregate per asset: collect_list(time) and collect_list of each depth column, ordered by bucket.
     5) Build depth-major "data" and write ONE JSON per asset for this window.
   """
   try:
       # ----- frequency name -----
       if time_window == 1800:
           freq = "30mins"
       elif time_window == 3600:
           freq = "60mins"
       elif time_window == 86400:
           freq = "1day"
       elif time_window == 604800:
           freq = "1week"
       elif time_window == 2592000:
           freq = "1month"
       else:
           raise ValueError(f"Unsupported time_window: {time_window}")
       # ----- 1) Pivot once for all assets -----
       pivoted_df = (
           df.groupBy("epoch_timestamp", "asset_id")
             .pivot("fiber_length")
             .agg(F.first("temperature"))
       )
       # Depth column discovery from pivoted schema (numeric-looking column names only)
       meta_cols = {"epoch_timestamp", "asset_id"}
       depth_cols = []
       for c in pivoted_df.columns:
           if c in meta_cols:
               continue
           try:
               float(c)  # keep numeric-looking names
               depth_cols.append(c)
           except Exception:
               pass
       if not depth_cols:
           logger.warning("No depth columns detected after pivot; nothing to publish.")
           return
       # Stable numeric ordering for depths (as strings)
       try:
           depth_cols = sorted(depth_cols, key=lambda x: float(x))
       except Exception:
           depth_cols = sorted(depth_cols)
       # ----- 2) Bucket per asset -----
       w_ordered = Window.partitionBy("asset_id").orderBy(F.col("epoch_timestamp").asc())
       bucketed_df = pivoted_df.withColumn("bucket", F.ntile(sample_size).over(w_ordered))
       # ----- 3) Aggregate per (asset_id, bucket) -----
       avg_exprs = [F.avg(F.col(f"`{c}`")).alias(c) for c in depth_cols]
       per_bucket = (
           bucketed_df.groupBy("asset_id", "bucket")
                      .agg(
                          F.min("epoch_timestamp").alias("first_timestamp"),
                          *avg_exprs
                      )
       )
       # ----- 4) Order by bucket and aggregate PER ASSET into multi-time arrays -----
       # time = collect_list(first_timestamp) ordered by bucket asc
       # depth arrays = collect_list(<depth col>) ordered by bucket asc
       ordered = (
           per_bucket
           .orderBy("bucket")
           .groupBy("asset_id")
           .agg(
               F.collect_list("first_timestamp").alias("time"),
               *[F.collect_list(F.col(f"`{c}`")).alias(f"arr__{i}") for i, c in enumerate(depth_cols)]
           )
       )
       # Build 'depth' and 'data' columns: depth is array of labels; data is array-of-arrays in depth order
       depth_lits = F.array(*[F.lit(c) for c in depth_cols])
       data_arrays = F.array(*[F.col(f"arr__{i}") for i, _ in enumerate(depth_cols)])
       final = (
           ordered
           .withColumn("depth", depth_lits)
           .withColumn("data",  data_arrays)
           .select("asset_id", "time", "depth", "data")
       )
       # Collect once per asset (one JSON per asset)
       rows = final.collect()
       if not rows:
           logger.info("No aggregated rows to publish.")
           return
       # ----- 5) Prepare per-asset tasks -----
       tasks = []
       for r in rows:
           tasks.append({
               "asset_id": r["asset_id"],
               "time":     r["time"]  or [],
               "depth":    r["depth"] or [],
               "data":     r["data"]  or []
           })
       logger.info(f"Prepared {len(tasks)} asset files; depths={len(depth_cols)}; sample_size={sample_size}")
       def publish_one(task):
           asset_id = task["asset_id"]
           time_arr = task["time"]
           depth_arr = task["depth"]
           data_arr = task["data"]
           # Ensure depth-major data shape aligns with depth_arr length
           # (Should naturally, but keep a small guard)
           if len(data_arr) != len(depth_arr):
               # pad/trim defensively to avoid malformed JSON
               min_len = min(len(data_arr), len(depth_arr))
               data_arr = data_arr[:min_len]
               depth_arr = depth_arr[:min_len]
           json_obj = {
               "data":  data_arr,     # [[d1_t1, d1_t2, ...], [d2_t1, d2_t2, ...]]
               "time":  time_arr,     # [t1, t2, ...] ordered by bucket
               "depth": depth_arr     # [d1, d2, ...]
           }
           json_output = json.dumps(json_obj, ensure_ascii=False)
           # ONE file per asset for this window
           write_dir  = f"{target_file_path}/{asset_id}"
           file_stem  = f"{asset_id}_{epoch_start_time}_{freq}"
           local_json = f"/tmp/{file_stem}.json"
           local_zip  = f"/tmp/{file_stem}.zip"
           cloud_zip  = f"{write_dir}/{epoch_start_time}_{freq}.zip"
           try:
               # Write JSON locally
               with open(local_json, "w") as f:
                   f.write(json_output)
               # Zip locally
               with zipfile.ZipFile(local_zip, "w", compression=zipfile.ZIP_DEFLATED) as zipf:
                   zipf.write(local_json, arcname=f"{file_stem}.json")
               # Upload
               dbutils.fs.cp(f"file:{local_zip}", cloud_zip, True)
               return f"[{asset_id}] Uploaded: {cloud_zip}"
           except Exception as e:
               return f"[{asset_id}] FAILED: {e}"
           finally:
               # Cleanup
               for p in (local_json, local_zip):
                   try:
                       os.remove(p)
                   except Exception:
                       pass
       # ----- 6) Parallelize by asset (max 10 concurrent uploads) -----
       with ThreadPoolExecutor(max_workers=10) as ex:
           futures = [ex.submit(publish_one, t) for t in tasks]
           for fut in as_completed(futures):
               logger.info(fut.result())
   except Exception as e:
       logger.error(f"Error in dts_file_publish : {str(e)}")
       raise
