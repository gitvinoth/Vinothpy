
# COMMAND ----------

def get_rules_df(spark, logger, source_file_list):
    
    # This function reads the data from the json file list and returns a dataframe.
    
    try:
        logger.info(f"Reading the source files : {source_file_list}")
        conditions_schema = StructType(
            [
                StructField("condition_id", IntegerType(), True),
                StructField("condition_name", StringType(), True),
                StructField("asset_id", ArrayType(StringType()), True),
                StructField("join_condition", StringType(), True),
                StructField("parameter", StringType(), True),
                StructField("operator", StringType(), True),
                StructField("class", IntegerType(), True),
                StructField("threshold", DoubleType(), True),
                StructField("duration", IntegerType(), True),
                StructField("wire", StringType(), True),
                StructField("function", StringType(), True),
                StructField("wire_length_from", IntegerType(), True),
                StructField("wire_length_to", IntegerType(), True),
                StructField("rule_run_frequency", IntegerType(), True),
                StructField("sensor_type", StringType(), True),
                StructField("severity", StringType(), True),
                StructField("risk_register_controls", ArrayType(IntegerType()), True),
                StructField("baseline_time", IntegerType(), True),
                StructField("threshold_unit", StringType(), True),
            ]
        )

        schema = StructType(
            [
                StructField("rule_id", IntegerType()),
                StructField("rule_name", StringType(), True),
                StructField("tenant_id", StringType(), True),
                StructField("conditions", ArrayType(conditions_schema), True),
                StructField("operation", StringType(), True),
            ]
        )

        rules_df = read_json(spark, logger, source_file_list, schema).withColumn(
            "input_file", input_file_name()
        )

        return rules_df

    except Exception as e:
        logger.error(f"Error in get_rules_df() : {str(e)}")
        raise

# COMMAND ----------

def keep_latest_rules(logger, rules_df):
    # This function keeps only latest record for each rule id based on epoch timestamp extracted from file name.
    
    try:
        rules_df = add_epoch_timestamp(logger, rules_df)
        window_spec = Window.partitionBy("rule_id").orderBy(desc("epoch_timestamp"))

        rules_df = (
            rules_df.withColumn("row_num", row_number().over(window_spec))
            .where(col("row_num") == 1)
            .drop("row_num", "epoch_timestamp")
        )

        return rules_df

    except Exception as e:
        logger.error(f"Error in keep_latest_rules() : {str(e)}")
        raise

# COMMAND ----------

def delete_rules(spark, logger, rules_df, bronze_table_name):
    
    # This function deletes the rules from the rules bronze table for records with operation as delete.
    
    try:
        rules_delete_df = rules_df.filter(
            lower(trim(col("operation"))) == "delete"
        ).distinct()

        if rules_delete_df.count() > 0:
            logger.info("Found rules to Delete. Deleting the rules from delta table...")
            rules_dt = read_delta_table(spark, logger, bronze_table_name)

            (
                rules_dt.alias("target")
                .merge(
                    rules_delete_df.alias("source"), "target.rule_id = source.rule_id"
                )
                .whenMatchedDelete()
                .execute()
            )

        else:
            logger.info("No rules to delete...")

    except Exception as e:
        logger.error(f"Error in delete_rules() : {str(e)}")
        raise

# COMMAND ----------

def apply_join_condition(logger, rules_df):
    
    # This function applies the 'and' and 'or' join condition on the rules with operation as create/update and returns the resulting dataframe.
    
    try:
        logger.info(f"Extracting the create/update rules...")
        create_update_rules_df = rules_df.filter(
            col("operation") != "delete"
        ).distinct()

        logger.info("Exploding conditions column and extracting values...")
        create_update_rules_df = (
            create_update_rules_df.withColumn("conditions", explode(col("conditions")))
            .withColumn("condition_id", col("conditions")["condition_id"])
            .withColumn("condition_name", col("conditions")["condition_name"])
            .withColumn("asset_id", col("conditions")["asset_id"])
            .withColumn("join_condition", col("conditions")["join_condition"])
            .withColumn("parameter", col("conditions")["parameter"])
            .withColumn("operator", col("conditions")["operator"])
            .withColumn("class", col("conditions")["class"])
            .withColumn("threshold", col("conditions")["threshold"])
            .withColumn("duration", col("conditions")["duration"])
            .withColumn("wire", col("conditions")["wire"])
            .withColumn("function", col("conditions")["function"])
            .withColumn("wire_length_from", col("conditions")["wire_length_from"])
            .withColumn("wire_length_to", col("conditions")["wire_length_to"])
            .withColumn("rule_run_frequency", col("conditions")["rule_run_frequency"])
            .withColumn("sensor_type", col("conditions")["sensor_type"])
            .withColumn("severity", col("conditions")["severity"])
            .withColumn(
                "risk_register_controls", col("conditions")["risk_register_controls"]
            )
            .withColumn("baseline_time", col("conditions")["baseline_time"])
            .withColumn("threshold_unit", col("conditions")["threshold_unit"])"
            .withColumn("query", lit("query"))
            .withColumn("last_updated_date", lit(datetime.now()))
            .drop(col("conditions"))
        )
        if create_update_rules_df.filter(col("join_condition") != "").count() > 0:
            logger.info("Exploding asset_id column for join condition as 'or'...")
            create_update_rules_or_df = create_update_rules_df.filter(
                (lower(trim(col("join_condition"))) == "or")
            ).withColumn("asset_id", explode(col("asset_id")))
            logger.info("Concatenating asset_id column for join condition as 'and'...")
            create_update_rules_and_df = create_update_rules_df.filter(
                (lower(trim(col("join_condition"))) == "and")
            ).withColumn("asset_id", concat_ws(",", col("asset_id")))
            rules_df = create_update_rules_and_df.union(create_update_rules_or_df)

            logger.info(
                "Generating asset_id column as an array of string from string..."
            )
        else:
            logger.info("Generating df for non null join condition...")
            rules_df = create_update_rules_df.withColumn(
                "asset_id", concat_ws(",", col("asset_id"))
            )

        return rules_df.withColumn("asset_id", split("asset_id", ","))

    except Exception as e:
        logger.error(f"Error in apply_join_condition() : {str(e)}")
        raise

# COMMAND ----------

def get_rules_to_upsert(logger, rules_df, rules_dt):
    
    # This function returns a dataframe containing rules that needs to be created/inserted and updated.
    # Note : It only retains rules with create operation which are not present in the rules bronze table.
    
    try:
        rules_dt_df = rules_dt.toDF()

        logger.info(
            "Filtering rules where operation is 'create' and rule_id is not present in bronze table..."
        )

        create_src = rules_df.filter(lower(trim(col("operation"))) == "create")

        # A left_anti join returns all rows from the left DataFrame create_src that do not have a match in the right DataFrame rules_dt_df. In this case, it returns all rules in create_src whose rule_id does not exist in rules_dt_df.
        rules_create_df = create_src.join(
            rules_dt_df, create_src.rule_id == rules_dt_df.rule_id, how="left_anti"
        )
        logger.info("Filtering rules where operation is 'update'...")

        rules_update_df = rules_df.filter(lower(trim(col("operation"))) == "update")

        return rules_create_df.union(rules_update_df)

    except Exception as e:
        logger.error(f"Error in get_rules_to_upsert() : {str(e)}")
        raise

# COMMAND ----------

def add_data_frequency(logger, rules_df, asset_df):
    
    # This function adds data frequnecy column from asset bronze table in the rules dataframe.
    
    try:
        logger.info("Fetching asset_id and data_frequency from asset bronze table...")
        asset_df = asset_df.select(col("asset_id"), col("data_frequency")).withColumn(
            "data_frequency", 2 * col("data_frequency") - 1
        )

        # Add a unique id to each row in rules_df. This is being used instead of using business key (asset_id, rule_id, condition_id) because for 'Or' it is exploded before

        logger.info("Adding an unique id in each row of rules...")
        rules_df = rules_df.withColumn("id", monotonically_increasing_id())

        # Explode the asset_id array in rules_df

        exploded_rules_df = rules_df.select(
            "id", explode(rules_df.asset_id).alias("asset_id")
        )

        logger.info("Joining rules and asset data based on asset_id...")
        joined_df = exploded_rules_df.join(asset_df, on="asset_id", how="left")

        # Group by id and collect data_frequency into a list

        logger.info("Collecting data frequency in a list...")
        result_df = joined_df.groupBy("id").agg(
            collect_list("data_frequency").alias("data_frequency"),
            ((max("data_frequency") + 1) / 2)
            .cast("integer")
            .alias("max_data_frequency"),
        )

        # Join back with rules_df to get the original asset_id array
        
        logger.info("Adding data_frequency column...")
        result_df = rules_df.join(result_df, on="id", how="inner")

        result_df = result_df.withColumn(
            "window_slide_duration",
            when(col("function") == "average", col("duration")).otherwise(lit(0))
            )

        return result_df

    except Exception as e:
        logger.error(f"Error in add_data_frequency() : {str(e)}")
        raise

# COMMAND ----------

@udf(returnType=StringType())
def replace_parameter(query, parameter):
    if parameter in DSS_PARAMETER_MAPPING.keys():
        return query.replace("$parameter", DSS_PARAMETER_MAPPING[parameter])
    return query.replace("$parameter", parameter)

# COMMAND ----------

def add_rule_query(logger, rules_df):
    
    #This function generates rule SQL query as per the template and return two dataframes - one containing the generated rules and one containing the malformed generated rules.
    
    try:
        # Generating Query based on asset id list. UDF accepts array of string where each element of the array would be utilized for query generation.

        logger.info("Generating rule query...")
        rules_df = rules_df.withColumn(
            "query",
            lit(
                generate_query_udf(
                    col("asset_id"),
                    col("parameter"),
                    col("class"),
                    col("data_frequency"),
                    col("baseline_timestamp_list"),
                    col("function"),
                    col("wire"),
                    col("wire_length_from"),
                    col("wire_length_to"),
                    col("duration"),
                    col("join_condition"),
                    col("operator"),
                    col("threshold"),
                    col("threshold_unit")
                    
                )
            ),
        )
        logger.info("Filtering errorneous records...")
        rules_null_df = rules_df.filter(col("query") == "data_frequency_null")

        logger.info("Filtering records with generated query...")
        rules_df = rules_df.filter(col("query") != "data_frequency_null")

        logger.info("Replacing parameters in query with values...")

        rules_df = (
            rules_df.withColumn(
                "query", replace_parameter(col("query"), col("parameter"))
            )
            .withColumn("query", expr("replace(query,'$operator', operator)"))
            .withColumn(
                "query",
                when(
                    col("query").contains("$duration"),
                    expr("replace(query,'$duration', duration)"),
                ).otherwise(col("query")),
            )
            .withColumn("query", expr("replace(query,'$threshold', threshold)"))
            .withColumn(
                "query",
                when(
                    col("query").contains("$class"),
                    expr("replace(query,'$class', class)"),
                ).otherwise(col("query")),
            )
        )

        return rules_df, rules_null_df

    except Exception as e:
        logger.error(f"Error in add_rule_query() : {str(e)}")
        raise

# COMMAND ----------

def log_error_rules(
    logger,
    error_rules_df,
    bronze_error_log,
    job_id,
    run_id,
    task_id,
    workflow_name,
    task_name,
):
    
    # This function logs the malformed rules in the error log table
    
    try:
        error_rules_df = (
            error_rules_df.select("asset_id", "input_file")
            .withColumn("workflow_job_id", lit(job_id))
            .withColumn("run_id", lit(run_id))
            .withColumn("task_id", lit(task_id))
            .withColumn("workflow_name", lit(workflow_name))
            .withColumn("task_name", lit(task_name))
            .withColumn("source", col("input_file"))
            .withColumn(
                "error_message",
                concat(
                    lit(
                        "Data Frequency missing in asset table for the asset_id/ some of the the asset_ids (in case of 'AND'). List of Asset id/s: "
                    ),
                    regexp_replace(concat_ws(", ", col("asset_id")), "^,\\s", ""),
                ),
            )
            .withColumn("additional_context", lit("NA"))
            .withColumn("last_updated_date", current_timestamp())
            .drop("asset_id", "input_file")
        )

        logger.info(f"Writing malformed rules into {bronze_error_log} table...")
        write_table(logger, error_rules_df, "append", bronze_error_log)

    except Exception as e:
        logger.error(f"Error in log_error_rules() : {str(e)}")
        raise

# COMMAND ----------

def delete_rules_with_update(logger, rules_df, rules_dt):
    """
    This function deletes rules from the bronze table where operation is 'update'.
    Note: Here, update is treated as delete and insert.
    """
    try:
        logger.info(
            "Deleting the rules from bronze table where operation == 'update' ..."
        )

        # Merge the rules_src_df with the bronze table rules_dt, deleting rows with matching rule_id where the operation is "update"
        # Mark the matched rows for deletion and insert the new rows
        (
            rules_dt.alias("tgt")
            .merge(
                rules_df.alias("src"),
                "src.rule_id = tgt.rule_id and src.operation = 'update'",
            )
            .whenMatchedDelete()
            .execute()
        )

    except Exception as e:
        logger.error(f"Error in delete_rules_with_update() : {str(e)}")
        raise

# COMMAND ----------

def write_rules(logger, rules_df, bronze_table_name):
    """
    This function writes the rule in the bronze table.
    """
    try:
        logger.info("Removing operation, id, data_frequency and input_file column...")
        rules_df = rules_df.drop(
            col("operation"), col("id"), col("data_frequency"), col("input_file")
        )

        logger.info("Rearranging columns to match the bronze table column sequence...")
        rules_df = rules_df.select(
            "rule_id",
            "rule_name",
            "tenant_id",
            "condition_id",
            "condition_name",
            "asset_id",
            "join_condition",
            "parameter",
            "operator",
            "class",
            "threshold",
            "duration",
            "wire",
            "function",
            "wire_length_from",
            "wire_length_to",
            "rule_run_frequency",
            "max_data_frequency",
            "sensor_type",
            "severity",
            "risk_register_controls",
            "baseline_time",
            "threshold_unit",
            "window_slide_duration"
            "query",
            "last_updated_date",
        )

        logger.info("Inserting the rules from source file...")
        write_table(logger, rules_df, "append", bronze_table_name)

    except Exception as e:
        logger.error(f"Error in write_rules() : {str(e)}")
        raise

# COMMAND ----------

def load_rule_bronze(
    spark,
    logger,
    source_file_list,
    bronze_table_name,
    bronze_asset_table,
    bronze_error_log,
    job_id,
    run_id,
    task_id,
    workflow_name,
    task_name,
) -> bool:
    try:
        rules_df = get_rules_df(spark, logger, source_file_list)

        if rules_df.count() == 0:
            logger.info("No rules to create/update/delete...")
            return False

        rules_df = keep_latest_rules(logger, rules_df)

        delete_rules(spark, logger, rules_df, bronze_table_name)

        rules_df = apply_join_condition(logger, rules_df)

        if rules_df.count() == 0:
            logger.info("No rules to create/update...")
            return True

        rules_dt = read_delta_table(spark, logger, bronze_table_name)

        rules_df = get_rules_to_upsert(logger, rules_df, rules_dt)

        asset_df = read_table(spark, logger, bronze_asset_table)

        rules_df = add_data_frequency(logger, rules_df, asset_df)

        rules_df, error_rules_df = add_rule_query(logger, rules_df)

        log_error_rules(
            logger,
            error_rules_df,
            bronze_error_log,
            job_id,
            run_id,
            task_id,
            workflow_name,
            task_name,
        )

        delete_rules_with_update(logger, rules_df, rules_dt)

        write_rules(logger, rules_df, bronze_table_name)

        return True

    except Exception as e:
        logger.error(f"Error in load_rule_bronze : {str(e)}")
        raise
