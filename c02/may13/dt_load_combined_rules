# Databricks notebook source
from datetime import datetime
from itertools import product
from math import factorial
from pyspark.sql.functions import (
   col, count, expr, collect_list, row_number,
   current_timestamp, monotonically_increasing_id, array, explode, concat_ws
)
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
from delta.tables import DeltaTable

# COMMAND ----------

# MAGIC %run ../utils/logger

# COMMAND ----------

# MAGIC %run ../utils/read_utility

# COMMAND ----------

# MAGIC %run ../utils/write_utility

# COMMAND ----------

# MAGIC %run ../utils/file_metadata_utility

# COMMAND ----------

# Load rules table
rules_df = spark.read.table("`ccus_dd40badc-44a8-4c24-994a-ab80edc83478_dev_03`.bronze_zone.rules")

# COMMAND ----------

rules_df.display()

# COMMAND ----------

# Step 2: Filter only multi-condition rule IDs
multi_condition_rule_ids = rules_df.groupBy("rule_id").agg(count("*").alias("cnt")).filter("cnt > 1").select("rule_id")


# COMMAND ----------

multi_condition_rule_ids.display()

# COMMAND ----------

# Step 3: Extract relevant rules and deduplicate (by sensor_type + asset_id + query)
multi_condition_rules_df = rules_df.join(multi_condition_rule_ids, on="rule_id", how="inner").dropDuplicates(["rule_id", "sensor_type", "asset_id", "query"])

# COMMAND ----------

multi_condition_rules_df.display()

# COMMAND ----------

exploded_df = multi_condition_rules_df.withColumn("asset_id_exploded", explode(col("asset_id")))

# COMMAND ----------

exploded_df.display()

# COMMAND ----------

# Step 4: Group queries for each sensor_type-asset_id combination
grouped_df = exploded_df.groupBy("rule_id", "sensor_type", "asset_id_exploded") \
   .agg(collect_list("query").alias("queries"))

# COMMAND ----------

grouped_df.display()

# COMMAND ----------

join_condition_map = rules_df.select("rule_id", "join_condition") \
   .distinct() \
   .rdd.collectAsMap()  # dict: {rule_id: "and" / "or"}

# COMMAND ----------

display(join_condition_map)

# COMMAND ----------

pandas_df = grouped_df.toPandas()

# COMMAND ----------

# Prepare dictionary with join_condition per rule_id
results = []
for rule_id in pandas_df["rule_id"].unique():
   subset = pandas_df[pandas_df["rule_id"] == rule_id]
   query_map = {}
   for _, row in subset.iterrows():
       key = (row["sensor_type"], row["asset_id_exploded"])
       query_map[key] = row["queries"]
   # Fetch the appropriate logical operator
   join_condition = join_condition_map.get(int(rule_id), "and").lower()
   logical_op = "INTERSECT" if join_condition == "and" else "UNION ALL"
   value_lists = list(query_map.values())
   for combo in product(*value_lists):
       cleaned_combo = []
       for q in combo:
           q = q.strip()
           if q.lower().startswith("with") or "select" in q.lower():
               if "end_time" not in q.lower().split("from")[0]:
                   q_wrapped = f"SELECT start_time, '' AS end_time FROM ({q})"
               else:
                   q_wrapped = f"SELECT start_time, end_time FROM ({q})"
           else:
               q_wrapped = f"SELECT start_time, '' AS end_time FROM ({q})"
           cleaned_combo.append(q_wrapped)
       combined_query = f" {logical_op} ".join(cleaned_combo)
       results.append((int(rule_id), combined_query))

# COMMAND ----------

# Step 6: Create DataFrame from combinations
import numpy as np
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Convert numpy.int32 to standard Python int
converted_results = [(int(row[0]), row[1]) for row in results]

schema = StructType([
   StructField("rule_id", IntegerType(), True),
   StructField("combined_query", StringType(), True),
])
combo_df = spark.createDataFrame(results, schema)

# COMMAND ----------

combo_df.display()

# COMMAND ----------

window_spec = Window.partitionBy("rule_id").orderBy(monotonically_increasing_id())
combo_df = combo_df.withColumn("group_id", row_number().over(window_spec)) \
                   .withColumn("combined_query_updated_date", current_timestamp())

# COMMAND ----------

combo_df.display()

# COMMAND ----------

# Step 8: Expand each combined query to all asset_id + condition_id for that rule
condition_keys_df = rules_df.select("rule_id", "condition_id", "asset_id").distinct()
final_combined_df = condition_keys_df.join(combo_df, on="rule_id", how="inner")

# COMMAND ----------

window_spec = Window.partitionBy("rule_id", "condition_id", "asset_id").orderBy("combined_query_updated_date")
final_combined_df = final_combined_df.withColumn("row_num", row_number().over(window_spec))

# COMMAND ----------

last_condition_df = rules_df.groupBy("rule_id").agg(expr("max(condition_id)").alias("last_condition_id"))
final_combined_df = final_combined_df.join(last_condition_df, on="rule_id", how="left")

# COMMAND ----------

final_combined_df.display()

# COMMAND ----------

filtered_combined_df = final_combined_df.filter(
   (col("row_num") == 1) | (col("condition_id") == col("last_condition_id"))
)

#.drop("row_num", "last_condition_id", "asset_id")

# COMMAND ----------

filtered_combined_df.select(
    "rule_id", 
    "condition_id", 
    "group_id", 
    "combined_query", 
    col("combined_query_updated_date").alias("last_updated_date")
).write.mode("overwrite").saveAsTable("`ccus_dd40badc-44a8-4c24-994a-ab80edc83478_dev_03`.bronze_zone.combined_rules")

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM `ccus_dd40badc-44a8-4c24-994a-ab80edc83478_dev_03`.bronze_zone.combined_rules
