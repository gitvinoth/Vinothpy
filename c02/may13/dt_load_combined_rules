# Databricks notebook source
from datetime import datetime
from itertools import product
from math import factorial
from pyspark.sql.functions import (
   col, count, expr, collect_list, row_number,
   current_timestamp, monotonically_increasing_id, array, explode, concat_ws
)
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
from delta.tables import DeltaTable

# COMMAND ----------

# MAGIC %run ../utils/logger

# COMMAND ----------

# MAGIC %run ../utils/read_utility

# COMMAND ----------

# MAGIC %run ../utils/write_utility

# COMMAND ----------

# MAGIC %run ../utils/file_metadata_utility

# COMMAND ----------

# Load rules table
rules_df = spark.read.table("`ccus_dd40badc-44a8-4c24-994a-ab80edc83478_dev_03`.bronze_zone.rules")

# COMMAND ----------

rules_df.display()

# COMMAND ----------

# Step 2: Filter only multi-condition rule IDs
multi_condition_rule_ids = rules_df.groupBy("rule_id").agg(count("*").alias("cnt")).filter("cnt > 1").select("rule_id")


# COMMAND ----------

multi_condition_rule_ids.display()

# COMMAND ----------

# Step 3: Extract relevant rules and deduplicate (by sensor_type + asset_id + query)
multi_condition_rules_df = rules_df.join(multi_condition_rule_ids, on="rule_id", how="inner").dropDuplicates(["rule_id", "sensor_type", "asset_id", "query"])

# COMMAND ----------

multi_condition_rules_df.display()

# COMMAND ----------

exploded_df = multi_condition_rules_df.withColumn("asset_id_exploded", explode(col("asset_id")))

# COMMAND ----------

exploded_df.display()

# COMMAND ----------

# Step 4: Group queries for each sensor_type-asset_id combination
grouped_df = exploded_df.groupBy("rule_id", "sensor_type", "asset_id_exploded") \
   .agg(collect_list("query").alias("queries"))

# COMMAND ----------

grouped_df.display()

# COMMAND ----------

pandas_df = grouped_df.toPandas()
results = []
for rule_id in pandas_df["rule_id"].unique():
   subset = pandas_df[pandas_df["rule_id"] == rule_id]
   query_groups = {}
   for _, row in subset.iterrows():
       sensor_key = f"{row['sensor_type']}|{','.join(row['asset_id_exploded']) if isinstance(row['asset_id_exploded'], list) else row['asset_id_exploded']}"
       query_groups.setdefault(sensor_key, []).extend(row["queries"])
   value_lists = list(query_groups.values())
   for combo in product(*value_lists):
       wrapped_queries = []
       for q in combo:
           q = q.strip()
           q_lower = q.lower()
           if q_lower.startswith("with"):
               wrapped_queries.append(f"SELECT start_time, end_time FROM ({q})")
           elif "end_time" not in q_lower.split("from")[0]:
               wrapped_queries.append(f"SELECT start_time, '' AS end_time FROM ({q})")
           else:
               wrapped_queries.append(q)
       combined_query = " INTERSECT ".join(wrapped_queries)
       results.append((int(rule_id), combined_query))

# COMMAND ----------

# Step 6: Create DataFrame from combinations
import numpy as np
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Convert numpy.int32 to standard Python int
converted_results = [(int(row[0]), row[1]) for row in results]

schema = StructType([
   StructField("rule_id", IntegerType(), True),
   StructField("combined_query", StringType(), True),
])
combo_df = spark.createDataFrame(results, schema)

# COMMAND ----------

combo_df.display()

# COMMAND ----------

# Step 7: Add group_id and timestamp
window_spec = Window.partitionBy("rule_id").orderBy(monotonically_increasing_id())
combo_df = combo_df.withColumn("group_id", row_number().over(window_spec)) \
                  .withColumn("combined_query_updated_dte", current_timestamp())

# COMMAND ----------

combo_df.display()

# COMMAND ----------

# Step 8: Expand each combined query to all asset_id + condition_id for that rule
condition_keys_df = rules_df.select("rule_id", "condition_id", "asset_id").distinct()
final_combined_df = condition_keys_df.join(combo_df, on="rule_id", how="inner")

# COMMAND ----------

final_combined_df.display()

# COMMAND ----------

from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

# Define a window specification
window_spec = Window.partitionBy("rule_id", "condition_id", "asset_id").orderBy("combined_query_updated_dte")

# Add a row number to each row within the window
final_combined_df = final_combined_df.withColumn("row_num", row_number().over(window_spec))

# COMMAND ----------

final_combined_df.display()

# COMMAND ----------



# COMMAND ----------

