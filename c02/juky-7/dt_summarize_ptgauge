# Databricks notebook source
import os
import time
from datetime import datetime
from pyspark.sql import DataFrame
from pyspark.sql.functions import (
    col,
    min,
    max,
    sum as _sum,
    count,
    struct,
    floor,
    lit,
    unix_timestamp,
    date_trunc,
    current_timestamp,
)

# COMMAND ----------

# MAGIC %run ../utils/logger

# COMMAND ----------

# MAGIC %run ../utils/read_utility

# COMMAND ----------

# MAGIC %run ../utils/write_utility

# COMMAND ----------

def pt_gauge_bucket(df: DataFrame, bucket_size_seconds: int) -> DataFrame:
    """
    Filters for non-null asset_id, pressure, and temperature.
    Buckets by time interval (bucket_size_seconds) using epoch_timestamp.
    Filters rows where epoch_timestamp is within the current day's range.
    """
    current_day_start = unix_timestamp(date_trunc("DAY", current_timestamp()))
    current_ts = unix_timestamp(current_timestamp())
    return df.filter(
        col("asset_id").isNotNull()
        & col("pressure").isNotNull()
        & col("temperature").isNotNull()
        & (col("epoch_timestamp") >= current_day_start)
        & (col("epoch_timestamp") <= current_ts)
    ).withColumn(
        "bucket_start",
        floor(col("epoch_timestamp") / bucket_size_seconds) * bucket_size_seconds,
    )

# COMMAND ----------

def get_min_max(spark, logger, bucketed_df) -> DataFrame:
    try:
        logger.info("Creating min_max_df...")
        min_max_df = (
            bucketed_df.groupBy("asset_id", "bucket_start")
            .agg(
                min(struct(col("temperature"), col("epoch_timestamp"))).alias(
                    "min_temp_struct"
                ),
                max(struct(col("temperature"), col("epoch_timestamp"))).alias(
                    "max_temp_struct"
                ),
                min(struct(col("pressure"), col("epoch_timestamp"))).alias(
                    "min_pressure_struct"
                ),
                max(struct(col("pressure"), col("epoch_timestamp"))).alias(
                    "max_pressure_struct"
                ),
            )
            .select(
                col("asset_id"),
                col("bucket_start"),
                col("min_temp_struct.temperature").alias("min_temperature"),
                col("max_temp_struct.temperature").alias("max_temperature"),
                col("min_temp_struct.epoch_timestamp").alias("min_temp_timestamp"),
                col("max_temp_struct.epoch_timestamp").alias("max_temp_timestamp"),
                col("min_pressure_struct.pressure").alias("min_pressure"),
                col("max_pressure_struct.pressure").alias("max_pressure"),
                col("min_pressure_struct.epoch_timestamp").alias(
                    "min_pressure_timestamp"
                ),
                col("max_pressure_struct.epoch_timestamp").alias(
                    "max_pressure_timestamp"
                ),
            )
        )
        logger.info("Created min_max_df...")
        return min_max_df
    except Exception as e:
        logger.error(f"Error in get_min_max(): {e}")
        raise

# COMMAND ----------

def get_count(spark, logger, bucketed_df) -> DataFrame:
    try:
        logger.info("Creating count_df...")
        count_df = bucketed_df.groupBy("asset_id", "bucket_start").agg(
            count(col("temperature")).alias("count_temperature"),
            count(col("pressure")).alias("count_pressure"),
        )
        logger.info("Created count_df...")
        return count_df
    except Exception as e:
        logger.error(f"Error in get_count(): {e}")
        raise

# COMMAND ----------

def get_sum(spark, logger, bucketed_df) -> DataFrame:
    try:
        logger.info("Creating sum_df...")
        sum_df = bucketed_df.groupBy("asset_id", "bucket_start").agg(
            _sum(col("temperature")).alias("sum_temperature"),
            _sum(col("pressure")).alias("sum_pressure"),
        )
        logger.info("Created sum_df...")
        return sum_df
    except Exception as e:
        logger.error(f"Error in get_sum(): {e}")
        raise

# COMMAND ----------

def get_gold(spark, logger, min_max_df, sum_df, count_df):
    try:

        min_temp_df = min_max_df.select(
            col("asset_id"),
            lit("temperature_min").alias("parameter"),
            col("min_temp_timestamp").cast("long").alias("timestamp"),
            col("min_temperature").alias("value"),
        )

        max_temp_df = min_max_df.select(
            col("asset_id"),
            lit("temperature_max").alias("parameter"),
            col("max_temp_timestamp").cast("long").alias("timestamp"),
            col("max_temperature").alias("value"),
        )

        min_pressure_df = min_max_df.select(
            col("asset_id"),
            lit("pressure_min").alias("parameter"),
            col("min_pressure_timestamp").cast("long").alias("timestamp"),
            col("min_pressure").alias("value"),
        )

        max_pressure_df = min_max_df.select(
            col("asset_id"),
            lit("pressure_max").alias("parameter"),
            col("max_pressure_timestamp").cast("long").alias("timestamp"),
            col("max_pressure").alias("value"),
        )

        sum_temp_df = sum_df.select(
            col("asset_id"),
            lit("temperature_sum").alias("parameter"),
            col("bucket_start").cast("long").alias("timestamp"),
            col("sum_temperature").alias("value"),
        )

        sum_pressure_df = sum_df.select(
            col("asset_id"),
            lit("pressure_sum").alias("parameter"),
            col("bucket_start").cast("long").alias("timestamp"),
            col("sum_pressure").alias("value"),
        )

        count_temp_df = count_df.select(
            col("asset_id"),
            lit("temperature_count").alias("parameter"),
            col("bucket_start").cast("long").alias("timestamp"),
            col("count_temperature").alias("value"),
        )

        count_pressure_df = count_df.select(
            col("asset_id"),
            lit("pressure_count").alias("parameter"),
            col("bucket_start").cast("long").alias("timestamp"),
            col("count_pressure").alias("value"),
        )

        gold_df = (
            min_temp_df.union(max_temp_df)
            .union(min_pressure_df)
            .union(max_pressure_df)
            .union(sum_temp_df)
            .union(sum_pressure_df)
            .union(count_temp_df)
            .union(count_pressure_df)
        )

        gold_df = gold_df.select("asset_id", "parameter", "timestamp", "value")
        logger.info("Created gold_df...")

        return gold_df

    except Exception as e:
        logger.error(f"Error in get_gold: {e}")
        raise

# COMMAND ----------

def pt_gauge_data_summarize(
    spark,
    logger,
    pt_gauge_silver_table,
    pt_gauge_summary_table,
    bucket_size_seconds: int = 60,
) -> bool:
    try:
        pt_gauge_silver_df = read_table(spark, logger, pt_gauge_silver_table)

        if (pt_gauge_silver_df is None) or (pt_gauge_silver_df.count() == 0):
            logger.info(f"Error: No data found in table {pt_gauge_silver_table}")
            return False
        else:
            bucketed_df = pt_gauge_bucket(pt_gauge_silver_df, bucket_size_seconds)
            pt_gauge_min_max_df = get_min_max(spark, logger, bucketed_df)
            pt_gauge_sum_df = get_sum(spark, logger, bucketed_df)
            pt_gauge_count_df = get_count(spark, logger, bucketed_df)
            pt_gauge_gold_df = get_gold(
                spark, logger, pt_gauge_min_max_df, pt_gauge_sum_df, pt_gauge_count_df
            )

            logger.info(f"Writing the data to gold table... {pt_gauge_summary_table}")
            write_table(logger, pt_gauge_gold_df, "append", pt_gauge_summary_table)
            return True
    except Exception as e:
        logger.error(f"Error in pt_gauge_data_summarize(): {e}")
        raise
