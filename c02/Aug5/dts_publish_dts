# Databricks notebook source
from pyspark.sql.functions import (
    flatten,
    collect_list,
    array,
    first,
    struct,
    col,
    udf,
    floor,
    array_min,
    split,
    explode,
    posexplode,
    avg,
    sort_array,
    size,
    array_sort,
    round,
)
from datetime import datetime, timedelta
from pyspark.sql import DataFrame
from delta import DeltaTable
from pyspark.sql.types import (
    ArrayType,
    DoubleType,
    IntegerType,
    FloatType,
    StringType,
    LongType,
    StructType,
    StructField,
)
from pyspark.sql.window import Window
from datetime import datetime, timedelta, time as t
from pyspark.sql import functions as F
import numpy as np
from pyspark.sql import types as T
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf

# COMMAND ----------

import bz2
import calendar
import gzip
import json
import logging
import os
import tarfile
import time
import zipfile

from unittest.mock import Mock, patch, MagicMock
import pyspark.sql.functions as F

# COMMAND ----------

try:
    if dbutils:
        pass
except NameError:
    from src.utils.read_utility import read_delta_table
    from src.utils.on_premise.utility_functions import copy_file
    from src.utils.on_premise.utility_functions import list_files

# COMMAND ----------

def dts_data_filter(spark, logger, latest_data_time: int, time_window: int, silver_table_name: str):
    """
    This function filters the data from the silver table based on latest_data_time and time window and returns the resultant dataframe.
    """
    try:
        logger.info("Reading the dts delta table...")

        silver_table_df = spark.read.table(silver_table_name)

        if time_window == 604800:
            start_datetime = datetime.fromtimestamp(latest_data_time)
            start_of_week = start_datetime - timedelta(days=start_datetime.weekday())
            start_datetime = start_of_week.replace(hour=0, minute=0, second=0, microsecond=0)
            start_time = int(start_datetime.timestamp())
            end_datetime = start_datetime + timedelta(days=6, hours=23, minutes=59, seconds=59)
            end_time = int(end_datetime.timestamp())
            logger.info(f"Filtering data with start_time: {start_time} and end_time: {end_time}")
            filtered_df = silver_table_df.filter(
                (F.col("timestamp") >= start_time) & (F.col("timestamp") <= end_time)
            )
        elif time_window == 2592000:
            start_datetime = datetime.fromtimestamp(latest_data_time)
            start_datetime = start_datetime.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
            start_time = int(start_datetime.timestamp())
            _, last_day = calendar.monthrange(start_datetime.year, start_datetime.month)
            end_datetime = start_datetime.replace(day=last_day, hour=23, minute=59, second=59, microsecond=999999)
            end_time = int(end_datetime.timestamp())
            logger.info(f"Filtering data with start_time: {start_time} and end_time: {end_time}")
            filtered_df = silver_table_df.filter(
                (F.col("timestamp") >= start_time) & (F.col("timestamp") <= end_time)
            )
        else:
            seed_time = latest_data_time % time_window
            start_time = latest_data_time - seed_time
            end_time = start_time + time_window - 1
            logger.info(f"Filtering data with start_time: {start_time} and end_time: {end_time}")
            filtered_df = silver_table_df.filter(
                (F.col("timestamp") >= start_time) & (F.col("timestamp") <= end_time)
            )
        return filtered_df

    except Exception as e:
        if "TableNotFoundException" in str(e):
            logger.error(f"Error in dts_data_filter | Table not found: {str(e)}")
            raise
        else:
            logger.error(f"Error in dts_data_filter : {str(e)}")
            raise

# COMMAND ----------

def dts_data_window_append(logger, dts_df_filtered: DataFrame, sample_size: int) -> DataFrame:
    """
    This function groups the data on "depth", "asset_id" and appends all the records within the same "depth", "asset_id". 
    It then creates an additional column called json_output where it stores the data in struct format. 
    This column is used for publishing the files.
    """
    try:
        # Get the distinct asset_id values from the table
        asset_ids = dts_df_filtered.select("asset_id").distinct().collect()
        logger.info(f"Total distinct asset_ids found: {len(asset_ids)}")

        if not asset_ids:
            logger.error("No asset IDs found in the filtered dataset.")
            return dts_df_filtered.limit(0) 

        result_df_list = []

        # Iterate over each asset_id in the DataFrame
        for asset_id in asset_ids:
            # Filter the table by asset_id
            new_df = dts_df_filtered.filter(dts_df_filtered["asset_id"] == asset_id[0])

            # Step 1: Pivot the DataFrame to have depth as columns
            pivoted_df = new_df.groupBy("timestamp", "asset_id") \
                .pivot("depth") \
                .agg(F.first("temperature"))

            # Step 2: Bucket the epoch timestamps
            window_spec = Window.partitionBy("asset_id").orderBy("timestamp")
            bucketed_df = pivoted_df.withColumn("bucket", F.ntile(sample_size).over(window_spec))

            # Select the first timestamp in each bucket for each asset_id
            timestamp_df = bucketed_df.withColumn(
                "first_timestamp", F.first("timestamp").over(Window.partitionBy("asset_id", "bucket"))
            ).drop("timestamp")

            float_column_names = sorted(
                [
                    float(col) for col in timestamp_df.columns 
                    if col not in ["first_timestamp", "asset_id", "bucket"]
                ]
            )

            # Convert columns to strings
            column_names = [str(col) for col in float_column_names]

            averaged_df = timestamp_df.groupBy("bucket", "asset_id") \
                .agg(
                    F.first("first_timestamp").alias("first_timestamp"),  # Pick the first timestamp of each bucket
                    *[F.avg(F.col(f"`{col}`")).alias(col) for col in column_names]  # Average each temperature column
                )

            collected_df = averaged_df.groupBy("asset_id") \
                .agg(
                    F.collect_list("first_timestamp").alias("time"),  # Collect list of first timestamps
                    F.array(*[F.lit(col) for col in column_names]).alias("depth"),  # Collect column names as depth
                    *[F.collect_list(F.col(f"`{col}`")).alias(col) for col in column_names]# Collect fiber length columns
                    
                )

            df_pivoted = collected_df.groupBy("time", "depth").agg(
                F.collect_list(F.array(*[F.col(f"`{col}`") for col in column_names])).alias("data")
            )

            final_df = df_pivoted.withColumn("data", F.flatten(F.col("data")))

            # Add asset_id column:
            final_df = final_df.withColumn("asset_id", F.lit(asset_id[0]))

            # Create json_output column with 'asset_id'
            output_json = final_df.withColumn(
                'json_output', F.struct(F.col('data'), F.col('time'), F.col('depth'))
            )

            result_df_list.append(output_json)

        # Union all the DataFrames in the list, ensuring there's data
        if result_df_list:
            result_df = result_df_list[0]
            for df in result_df_list[1:]:
                result_df = result_df.union(df)
            return result_df
        else:
            return dts_df_filtered.limit(0)  

    except Exception as e:
        logger.error(f"Error in dts_data_window_append: {str(e)}")
        raise


# COMMAND ----------

def is_end_of_6th_hour_window_dts(current_time_epoch: int) -> bool:
    """
    This function checks if the current running time is at the end of a 6-hour window for DTS.
    """
    current_time = datetime.fromtimestamp(current_time_epoch)
    return current_time.hour % 6 == 5 and current_time.minute > 30

# COMMAND ----------

def is_end_of_day_dts(current_time_epoch: int) -> bool:
    """
    This function checks if the current running time is at the end of the day for DTS.
    """
    current_time = datetime.fromtimestamp(current_time_epoch)
    return current_time.hour == 23 and current_time.minute > 30

# COMMAND ----------

def get_max_time_of_data_dts(spark, silver_table_name: str) -> datetime:
    """
    This function gets the maximum time of the current day's data from the silver table.
    """
    try:
        # Read the silver table
        silver_table_df = spark.read.table(silver_table_name)

        # Get the maximum time from the filtered data
        max_time_epoch = silver_table_df.agg(F.max("timestamp")).collect()[0][0]

        return datetime.fromtimestamp(max_time_epoch)

    except Exception as e:
        if "TableNotFoundException" in str(e):
            logger.error(
                f"Error in get_max_time_of_current_day | Table not found: {str(e)}"
            )
            raise
        else:
            logger.error(f"Error in get_max_time_of_current_day : {str(e)}")
            raise

# COMMAND ----------

def write_and_zip_json_data_dts(
    logger,
    json_data,
    asset_id,
    depth,
    epoch_start_time,
    freq,
    cloud_zip_file_path,
):
    try:
        # Sanitize asset_id for filenames
        safe_asset_id = asset_id.replace("/", "_").replace(" ", "_")
        json_filename = f"{safe_asset_id}_{freq}_{epoch_start_time}.json"
        zip_filename = json_filename.replace(".json", ".zip")
        local_json_path = f"/tmp/{json_filename}"
        local_zip_path = f"/tmp/{zip_filename}"
 
        # Write JSON data to a local file
        with open(local_json_path, "w") as f:
            f.write(json_data)
 
        # Create and write to zipfile
        with zipfile.ZipFile(local_zip_path, "w", zipfile.ZIP_DEFLATED, compresslevel=1) as zipf:
            zipf.write(local_json_path, arcname=json_filename)
 
        logger.info(
            f"Publishing the file for DTS asset_id: {asset_id} to location: {cloud_zip_file_path}"
        )
 
        # Upload zip file to cloud
        copy_file(f"file:{local_zip_path}", cloud_zip_file_path)
 
        # Cleanup local files
        os.remove(local_json_path)
        os.remove(local_zip_path)
    except Exception as e:
        logger.error(f"Error in write_and_zip_json_data_dts: {str(e)}")
        # Attempt cleanup even on error
        for path in [locals().get('local_json_path'), locals().get('local_zip_path')]:
            if path and os.path.exists(path):
                try:
                    os.remove(path)
                except Exception:
                    pass
        raise

# COMMAND ----------

def append_and_zip_json_data_dts(
    logger,
    cloud_zip_file_path,
    json_data,
    asset_id,
    depth,
    epoch_start_time,
    freq,
):
    try:

        # Safer and more consistent naming
        safe_asset_id = asset_id.replace("/", "_").replace(" ", "_")
        zip_filename = f"{safe_asset_id}_{depth}_{epoch_start_time}_{freq}.zip"
        json_filename = zip_filename.replace(".zip", ".json")
        extract_dir = f"/tmp/{safe_asset_id}_{depth}_{epoch_start_time}_{freq}"
        local_zip_path = f"{extract_dir}/{zip_filename}"
        local_json_path = f"{extract_dir}/{json_filename}"
        updated_zip_path = f"{extract_dir}/updated_{zip_filename}"

        os.makedirs(extract_dir, exist_ok=True)

        # Download zip
        logger.info(f"Downloading ZIP from {cloud_zip_file_path} to {local_zip_path}")
        copy_file(cloud_zip_file_path, f"file:{local_zip_path}")

        # Extract only the relevant JSON file
        with zipfile.ZipFile(local_zip_path, "r") as z:
            z.extract(json_filename, extract_dir)

        # Load and update JSON
        with open(local_json_path, "r") as file:
            existing_data = json.load(file)
        new_data = json.loads(json_data)
        existing_data["data"] = [
            x + y for x, y in zip(existing_data["data"], new_data["data"])
        ]
        existing_data["time"].extend(new_data.get("time", []))

        # Write updated JSON
        with open(local_json_path, "w") as file:
            json.dump(existing_data, file)

        # Re-zip
        with zipfile.ZipFile(updated_zip_path, "w") as zipf:
            zipf.write(local_json_path, arcname=json_filename)

        # Upload updated zip to cloud
        copy_file(f"file:{updated_zip_path}", cloud_zip_file_path)
        logger.info(f"Appended and published ZIP to: {cloud_zip_file_path}")

        # Cleanup
        os.remove(local_zip_path)
        os.remove(updated_zip_path)

    except Exception as e:
        logger.error(f"Error in append_and_zip_json_data_dts: {str(e)}")
        raise

# COMMAND ----------

def dts_file_publish(
    logger,
    input_df: DataFrame,
    target_file_path: str,
    epoch_start_time: int,
    time_window: int,
):
    """
    This function generates each file, zips the file, and publishes the file
    """
    try:
        if input_df.count() == 0:
            logger.info("Input DataFrame is empty. No data to publish.")
            return

        if time_window == 1800:
            freq = "30mins"
        elif time_window == 3600:
            freq = "60mins"
        elif time_window == 86400:
            freq = "1day"
        elif time_window == 604800:
            freq = "1week"
        elif time_window == 2592000:
            freq = "1month"

        # Function to convert row to JSON
        def row_to_json(row):
            return json.dumps(row.asDict(), ensure_ascii=False)

        # Iterate over each row in the DataFrame
        for row in input_df.collect():
            # Get the depth value and json_output from the row
            depth_val = row.depth
            json_output = row.json_output
            asset_id = row.asset_id

            # Construct the path where files would be published
            write_file_path = f"{target_file_path}/{asset_id}"

            # Convert the json_output to JSON format
            json_data = row_to_json(json_output)

            # Construct the json file path
            file_path = (
                f"{write_file_path}/{epoch_start_time}_{freq}.json"
            )

            # Construct the cloud file path with the final file name
            cloud_zip_file_path = (
                f"{write_file_path}/{epoch_start_time}_{freq}.zip"
            )

            if time_window in [604800, 2592000]:
                # Check if the cloud zip file path already exists
                file_exists = False
                try:
                    if cloud_zip_file_path.split("/")[-1] in list_files(
                        f"{write_file_path}/"
                    ):
                        file_exists = True
                except Exception as e:
                    if "FileNotFoundException" in str(e):
                        file_exists = False
                    else:
                        raise

                if file_exists:
                    append_and_zip_json_data_dts(
                        logger,
                        cloud_zip_file_path,
                        json_data,
                        asset_id,
                        depth_val,
                        epoch_start_time,
                        freq,
                    )
                    logger.info(f"File is appended to the existing file:{cloud_zip_file_path}")
                else:
                    # Construct the local file path where files will be written for zipping files locally
                    write_and_zip_json_data_dts(
                        logger,
                        json_data,
                        asset_id,
                        depth_val,
                        epoch_start_time,
                        freq,
                        cloud_zip_file_path,
                    )
                    logger.info(f"File is newly added to the:{cloud_zip_file_path} file")
            else:
                # Directly create the file in the cloud for other time windows

                write_and_zip_json_data_dts(
                    logger,
                    json_data,
                    asset_id,
                    depth_val,
                    epoch_start_time,
                    freq,
                    cloud_zip_file_path,
                )
                logger.info(f"File is newly added to the:{cloud_zip_file_path} file")

    except Exception as e:
        if "OSError" in str(e):
            logger.error(f"Error in dts_file_publish: {str(e)}")
            raise

        else:
            logger.error(f"Error in dts_file_publish : {str(e)}")
            raise

# COMMAND ----------

def current_window_start_time_dts(
    spark, logger, time_window: int, latest_data_time: int
) -> int:
    try:
        start_time_date = datetime.fromtimestamp(latest_data_time)
        start_time = latest_data_time

        if time_window in [1800, 3600, 86400]:
            seed_time = latest_data_time % time_window
            start_time = latest_data_time - seed_time
        elif time_window == 604800:
            start_of_this_week = start_time_date - timedelta(
                days=start_time_date.weekday()
            )  # Start of this week (Monday)
            start_of_this_week = start_of_this_week.replace(
                hour=0, minute=0, second=0
            )
            start_time = int(start_of_this_week.timestamp())
        elif time_window == 2592000:
            # Get the start of the month
            month_start = start_time_date.replace(
                day=1, hour=0, minute=0, second=0
            ).timestamp()
            start_time = int(month_start)
        if start_time == 0:
            raise ValueError("Invalid time window provided")

        logger.info(f"Current window start time is: {start_time}")
        return start_time
    
    except Exception as e:
        logger.error(f"Error in current_window_start_time_dts: {str(e)}")
        raise

# COMMAND ----------

# Function to truncate data based on the epoch time column ('timestamp')
def truncate_data(spark, logger, silver_table) -> None:
    """
    Delete previous month data from the silver table on the first monday of the current month.
    """
    try:
        logger.info("Starting data truncation process...")

        # Get the current date
        current_date = datetime.now()
        day = current_date.day
        day_of_week = current_date.weekday()
        is_within_time_window = t(0, 0) <= current_date.time() < t(0, 30)

        # Check if today is the first Monday of the current month
        if day_of_week == 0 and day <= 7 and is_within_time_window:
            logger.info(
                "Today is the first Monday of the month. Proceeding with truncation..."
            )
            first_day_of_month = current_date.replace(
                day=1, hour=0, minute=0, second=0, microsecond=0
            )
            # Get the epoch time for the first of the last month
            epoch_time = int(first_day_of_month.timestamp())

            # Perform the DELETE operation to remove rows older than the first of last month
            silver_table.delete(F.col("timestamp") < epoch_time)

        else:
            logger.info(
                "Today is not the first Monday of the month. Skipping truncation."
            )

    except Exception as e:
        logger.error(f"Error in truncate_data: {str(e)}")
        raise

# COMMAND ----------

def last_window_start_time_dts(
    spark, logger, current_time_epoch: int, time_window: int
) -> int:
    try:
        logger.info("Inside last_window_start_time function")
        start_time = 0
        evaluation_time = current_time_epoch - time_window
        # Converting to date format
        dt = datetime.fromtimestamp(current_time_epoch)
        start_time_date = datetime.fromtimestamp(current_time_epoch).date()
        if time_window in [1800, 3600]:
            seed_time = evaluation_time % time_window
            start_time = evaluation_time - seed_time
        elif time_window == 86400:
            current_datetime = datetime.now()
            is_within_time_window = t(0, 0) <= current_datetime.time() < t(0, 30)
            if is_within_time_window:
                previous_day = current_datetime.date() - timedelta(days=1)
                start_time = int(datetime.combine(previous_day, t(0, 0)).timestamp())
        elif time_window == 604800:
            current_datetime = datetime.now()
            is_monday = current_datetime.weekday() == 0
            is_within_time_window = t(0, 0) <= current_datetime.time() < t(0, 30)
            logger.info(
                f"Time window : {time_window} | is_monday : {is_monday} | is_within_time_window : {is_within_time_window}"
            )
            if is_monday and is_within_time_window:
                previous_week_start = current_datetime - timedelta(days=7)
                start_time = int(
                    previous_week_start.replace(
                        hour=0, minute=0, second=0, microsecond=0
                    ).timestamp()
                )
        elif time_window == 2592000:
            current_datetime = datetime.now()
            is_first_day_of_month = current_datetime.day == 1
            is_within_time_window = t(0, 0) <= current_datetime.time() < t(0, 30)
            logger.info(
                f"Time window : {time_window} | is_first_day_of_month : {is_first_day_of_month} | is_within_time_window : {is_within_time_window}"
            )
            if is_first_day_of_month and is_within_time_window:
                logger.info("Inside if")
                first_day_of_current_month = current_datetime.replace(
                    day=1, hour=0, minute=0, second=0, microsecond=0
                )
                last_day_of_previous_month = first_day_of_current_month - timedelta(
                    days=1
                )
                start_time = int(last_day_of_previous_month.replace(day=1).timestamp())

        return start_time

    except Exception as e:
        if "ZeroDivisionError" in str(e):
            logger.error(f"Error in last_window_start_time: {str(e)}")
            raise
        else:
            logger.error(f"Error in last_window_start_time : {str(e)}")
            raise

# COMMAND ----------

def get_cw_dts_df_filtered(
    spark,
    logger,
    latest_data_time: int,
    time_window: int,
    silver_table_name: str,
    sample_size: int,
):
    # Filter the data for the current window
    cw_dts_df_filtered = dts_data_filter(
        spark, logger, latest_data_time, time_window, silver_table_name
    )
    cw_df_append = dts_data_window_append(logger, cw_dts_df_filtered, sample_size)
    logger.info(
        f"Data found, current window execution will happen and total {cw_df_append.count()} files will be published"
    )
    return cw_df_append

# COMMAND ----------

def dts_data_publish(
    spark,
    logger,
    silver_table_name: str,
    target_file_path: str,
    initation_time: int,
    time_window: int,
    sample_size: int,
    engine_run_frequency: int,
):
    """
    This function is the main function which controls all the logic by calling other functions. First it calculates start time and using that it filters necessary data from silver table. After that, it appends necessary data by groups and creates the necessary struct format data. Finally, it generates the zip file and publishes it.
    """
    try:
        # Read the silver table into a DataFrame
        silver_table_dt = read_delta_table(spark, logger, silver_table_name)
        silver_table_df = silver_table_dt.toDF()

        logger.info(
            f"Starting execution for execution time : {initation_time} and time_window : {time_window}"
        )
        # Get the latest data time
        #latest_data_time = get_max_time_of_data_dts(spark, silver_table_name)

        if initation_time is None:
            logger.info("No data available for the current date.")
            return
        # Ensure latest_data_time is a Unix timestamp
        if isinstance(initation_time, datetime):
            latest_data_time = int(initation_time.timestamp())
        else:
            latest_data_time = initation_time

        if initation_time is not None:
            # Run always for the previous run cycle to accommodate all data received
            latest_data_time = initation_time - engine_run_frequency
            # Run weekly task only if latest data received has crossed 6 hour window

            if time_window == 604800:
                logger.info(
                    f"Publishing the file at the end of the 6th-hour window for Weekly Publish task and the end_of_6th_hour_window is : {latest_data_time} and time_window is: {time_window}"
                )

                # Calculate the window time as for file name
                cw_start_time = current_window_start_time_dts(
                    spark,
                    logger,
                    time_window,
                    latest_data_time,
                )

                # Get filtered and summarized df
                cw_df_append = get_cw_dts_df_filtered(
                    spark,
                    logger,
                    latest_data_time,
                    time_window,
                    silver_table_name,
                    sample_size,
                )

                # Publish the data
                dts_file_publish(
                    logger,
                    cw_df_append,
                    target_file_path,
                    cw_start_time,
                    time_window,
                )
            elif time_window == 2592000:

                logger.info(f"Publishing the file at the end of the day for Monthly publish task and end_of_day_time is: {latest_data_time} and time_window is: {time_window}")

                # Calculate the window time as for file name
                cw_start_time = current_window_start_time_dts(
                    spark,
                    logger,
                    time_window,
                    latest_data_time,
                )

                # Get filtered and summarized df
                cw_df_append = get_cw_dts_df_filtered(
                    spark,
                    logger,
                    latest_data_time,
                    time_window,
                    silver_table_name,
                    sample_size,
                )
                # Publish the data
                dts_file_publish(
                    logger,
                    cw_df_append,
                    target_file_path,
                    cw_start_time,
                    time_window,
                )

            elif time_window in [1800, 3600, 86400]:

                logger.info(f"Publishing the file for 30min, 1hr, 1day data for the latest_Date_time:{latest_data_time} and current running time_window is :{time_window}")

                # Calculate the window time as for file name
                cw_start_time = current_window_start_time_dts(
                    spark,
                    logger,
                    time_window,
                    latest_data_time,
                )

                # Get filtered and summarized df
                cw_df_append = get_cw_dts_df_filtered(
                    spark,
                    logger,
                    latest_data_time,
                    time_window,
                    silver_table_name,
                    sample_size,
                )
                # Publish the data
                dts_file_publish(
                    logger,
                    cw_df_append,
                    target_file_path,
                    cw_start_time,
                    time_window,
                )
            else:
                logger.info(f"No file publish for the current_time:{latest_data_time} and it is not the end of {time_window}")

        else:
            logger.info(f"No file publish for the current_time:{latest_data_time}")

    except Exception as e:
        if "FileNotFoundError" in str(e):
            logger.error(f"Error in dts_data_publish: {str(e)}")
            raise

        else:
            logger.error(f"Error in dts_data_publish : {str(e)}")
            raise
