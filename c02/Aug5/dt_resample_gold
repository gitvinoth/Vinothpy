# Databricks notebook source
from datetime import datetime
from pyspark.sql import DataFrame
from pyspark.sql.functions import (
    col, floor, explode, sequence, unix_timestamp, current_timestamp, date_trunc, lit,
    struct, min as _min, max as _max, count as _count, sum as _sum, expr,
    date_sub, date_add, unix_timestamp
)
from pyspark.sql.window import Window
from pyspark.sql import functions as F

# COMMAND ----------

# MAGIC %run ../utils/logger

# COMMAND ----------

# MAGIC %run ../utils/read_utility

# COMMAND ----------

# MAGIC %run ../utils/write_utility

# COMMAND ----------

try:
    if dbutils:
        pass # pragma: no cover
except NameError:
    from src.utils.write_utility import write_table
    from src.utils.read_utility import read_delta_table, read_table

# COMMAND ----------

def generate_bucket_times(spark, bucket_size_seconds: int) -> DataFrame:
    """Generate DataFrame with time buckets from yesterday's current time to now."""
    try:
        current_ts = unix_timestamp(current_timestamp()).cast("long")
        yesterday_same_time = (unix_timestamp(current_timestamp() - expr("INTERVAL 1 DAY"))).cast("long")
        return spark.sql("SELECT 1").select(
            explode(sequence(
                yesterday_same_time,
                current_ts,
                lit(bucket_size_seconds).cast("long")
            )).alias("bucket_start")
        ).withColumn(
            "bucket_start",
            (col("bucket_start") / bucket_size_seconds).cast("long") * bucket_size_seconds
        )
    except Exception as e:
        raise RuntimeError(f"generate_bucket_times failed: {e}")

# COMMAND ----------

def expand_buckets(bucket_times_df: DataFrame, base_df: DataFrame) -> DataFrame:
    """Cartesian join of asset/depth with bucket times."""
    try:
        asset_depth_df = base_df.select("asset_id", "depth").distinct()
        return asset_depth_df.crossJoin(bucket_times_df)
    except Exception as e:
        raise RuntimeError(f"expand_buckets failed: {e}")

# COMMAND ----------

def dts_silver_bucket(df: DataFrame, bucket_size_seconds: int) -> DataFrame:
    """Assign each record to its bucket."""
    try:
        current_day_start = unix_timestamp(current_timestamp() - expr("INTERVAL 1 DAY"))
        current_ts = unix_timestamp(current_timestamp())
        return df.filter(
            col("asset_id").isNotNull() &
            col("depth").isNotNull() &
            col("temperature").isNotNull() &
            (col("timestamp") >= current_day_start) &
            (col("timestamp") <= current_ts)
        ).withColumn(
            "bucket_start",
            (col("timestamp") - (col("timestamp") % bucket_size_seconds))
        )
    except Exception as e:
        raise RuntimeError(f"dts_bucket failed: {e}")

# COMMAND ----------

def dts_get_sum(bucketed_df: DataFrame, expanded_buckets: DataFrame) -> DataFrame:
    """Aggregate SUM(temperature) per bucket."""
    try:
        sum_df = bucketed_df.groupBy("asset_id", "depth", "bucket_start") \
            .agg(_sum("temperature").alias("sum_temperature"))
        return expanded_buckets.join(sum_df, ["asset_id", "depth", "bucket_start"], "left")
    except Exception as e:
        raise RuntimeError(f"dts_get_sum failed: {e}")

# COMMAND ----------

def dts_get_count(bucketed_df: DataFrame, expanded_buckets: DataFrame) -> DataFrame:
    """Aggregate COUNT(temperature) per bucket."""
    try:
        count_df = bucketed_df.groupBy("asset_id", "depth", "bucket_start") \
            .agg(_count("temperature").alias("count_temperature"))
        return expanded_buckets.join(count_df, ["asset_id", "depth", "bucket_start"], "left") \
            .fillna({"count_temperature": 0})
    except Exception as e:
        raise RuntimeError(f"dts_get_count failed: {e}")

# COMMAND ----------

def dts_get_min_max(bucketed_df: DataFrame, expanded_buckets: DataFrame) -> DataFrame:
    """Compute min/max temperature with timestamp per bucket."""
    try:
        min_max_df = bucketed_df.groupBy("asset_id", "depth", "bucket_start") \
            .agg(
                _min(struct(col("temperature"), col("timestamp"))).alias("min_temp_struct"),
                _max(struct(col("temperature"), col("timestamp"))).alias("max_temp_struct")
            ).select(
                "asset_id", "depth", "bucket_start",
                col("min_temp_struct.temperature").alias("min_temperature"),
                col("min_temp_struct.timestamp").alias("min_temperature_ts"),
                col("max_temp_struct.temperature").alias("max_temperature"),
                col("max_temp_struct.timestamp").alias("max_temperature_ts"),
            ).withColumn(
                "min_temperature_ts", _max("min_temperature_ts").over(Window.partitionBy("asset_id", "depth", "bucket_start"))
            ).withColumn(
                "max_temperature_ts", _max("max_temperature_ts").over(Window.partitionBy("asset_id", "depth", "bucket_start"))
            )
        return expanded_buckets.join(min_max_df, ["asset_id", "depth", "bucket_start"], "left")
    except Exception as e:
        raise RuntimeError(f"dts_get_min_max failed: {e}")

# COMMAND ----------

def dts_get_gold(
    min_max_df, sum_df, count_df, bucket_size_seconds, spark
):
    """
    Assemble the final gold summary table and return min, max, sum, count per asset_id/parameter/depth/bucket.
    """
    from pyspark.sql import functions as F

    try:
        min_max_with_count = min_max_df.join(
            count_df.select("asset_id", "depth", "bucket_start", "count_temperature"),
            on=["asset_id", "depth", "bucket_start"],
            how="left"
        )
        valid_min_max = min_max_with_count.filter(F.col("count_temperature") > 0)
        min_temp_df = valid_min_max.select(
            "asset_id", "depth",
            F.lit("temperature_min").alias("parameter"),
            F.col("min_temperature_ts").cast("long").alias("timestamp"),
            F.col("min_temperature").alias("value"),
            F.col("bucket_start")
        )
        max_temp_df = valid_min_max.select(
            "asset_id", "depth",
            F.lit("temperature_max").alias("parameter"),
            F.col("max_temperature_ts").cast("long").alias("timestamp"),
            F.col("max_temperature").alias("value"),
            F.col("bucket_start")
        )
        sum_temp_df = sum_df.select(
            "asset_id", "depth",
            F.lit("temperature_sum").alias("parameter"),
            F.col("bucket_start").cast("long").alias("timestamp"),
            F.col("sum_temperature").alias("value"),
            F.col("bucket_start")
        )
        count_temp_df = count_df.select(
            "asset_id", "depth",
            F.lit("temperature_count").alias("parameter"),
            F.col("bucket_start").cast("long").alias("timestamp"),
            F.col("count_temperature").alias("value"),
            F.col("bucket_start")
        )
        gold_df = min_temp_df.union(max_temp_df).union(sum_temp_df).union(count_temp_df) \
            .select("asset_id", "parameter", "timestamp", "depth", "value", "bucket_start") \
            .filter(
                F.col("value").isNotNull() &
                F.col("timestamp").isNotNull() &
                F.col("depth").isNotNull()
            )
        # Deduplicate: Only keep one row per asset_id/parameter/depth/bucket_start
        window = (
            Window.partitionBy("asset_id", "parameter", "depth", "bucket_start")
            .orderBy(
                # For min/max: if multiple rows (shouldn't usually happen), take first by timestamp
                F.col("timestamp").asc()
            )
        )
        gold_df = gold_df.withColumn("rn", F.row_number().over(window)).filter(F.col("rn") == 1).drop("rn")
        return gold_df.select("asset_id", "parameter", "timestamp", "value", "depth", "bucket_start")
    except Exception as e:
        raise RuntimeError(f"dts_get_gold failed: {e}")

# COMMAND ----------

def deduplicate_gold_table_dts(
    spark,
    logger,
    gold_table: str,
    gold_df: DataFrame,
    bucket_size_seconds: int
) -> None:
    """
    Deduplicate the gold summary table for DTS by:
    1. Aligning all input records to bucket_start.
    2. Extracting per-bucket min, max, sum, count correctly.
    3. Deleting overlapping records from gold table.
       - SUM/COUNT: by (asset_id, parameter, depth, bucket_start)
       - MIN/MAX: by (asset_id, depth) â€” full wipe to handle sparse buckets.
    4. Appending deduplicated DataFrame into the gold table.
    """
    try:
        logger.info("Aligning gold_df to bucket_start...")
        aligned_df = gold_df.withColumn(
            "bucket_start",
            (F.col("timestamp") / bucket_size_seconds).cast("long") * bucket_size_seconds
        )
        # Extract parameter-wise deduplicated values
        def get_min_max_sum_count(aligned_df, param, is_min=True):
            if param in ["temperature_min", "temperature_max"]:
                order_col = F.col("value").asc() if is_min else F.col("value").desc()
                ts_col = F.col("timestamp").asc() if is_min else F.col("timestamp").desc()
                window = Window.partitionBy("asset_id", "depth", "parameter", "bucket_start").orderBy(order_col, ts_col)
                return (
                    aligned_df.filter(F.col("parameter") == param)
                    .withColumn("rn", F.row_number().over(window))
                    .filter(F.col("rn") == 1)
                    .drop("rn")
                )
            else:
                return (
                    aligned_df.filter(F.col("parameter") == param)
                    .groupBy("asset_id", "depth", "parameter", "bucket_start")
                    .agg(
                        F.sum("value").alias("value"),
                        F.min("timestamp").alias("timestamp")
                    )
                )
        logger.info("Generating deduplicated parameter-wise frames...")
        min_df = get_min_max_sum_count(aligned_df, "temperature_min", is_min=True)
        max_df = get_min_max_sum_count(aligned_df, "temperature_max", is_min=False)
        sum_df = get_min_max_sum_count(aligned_df, "temperature_sum")
        count_df = get_min_max_sum_count(aligned_df, "temperature_count")
        # Align schema
        cols = ["asset_id", "parameter", "timestamp", "value", "depth", "bucket_start"]
        deduped_df = min_df.select(*cols).unionByName(
            max_df.select(*cols)
        ).unionByName(
            sum_df.select(*cols)
        ).unionByName(
            count_df.select(*cols)
        )
        logger.info("Registered deduped_gold_view")
        deduped_df.createOrReplaceTempView("deduped_gold_view")
        # Compute yesterday same time as epoch (int)
        current_day_start = spark.sql("SELECT unix_timestamp(date_sub(current_timestamp(), 1))").collect()[0][0]
        # Step 1: Delete SUM/COUNT using EXISTS
        delete_sum_count = f"DELETE FROM {gold_table} AS target WHERE target.parameter IN ('temperature_sum', 'temperature_count') AND target.timestamp >= {current_day_start} AND EXISTS (SELECT 1 FROM deduped_gold_view AS source WHERE source.parameter = target.parameter AND source.asset_id = target.asset_id AND source.depth = target.depth AND source.timestamp = target.timestamp)"
        spark.sql(delete_sum_count)
        logger.info("Deleted overlapping SUM/COUNT records.")
        # Step 2: Delete MIN/MAX for same asset_id, depth using EXISTS
        delete_min_max = f"DELETE FROM {gold_table} AS target WHERE target.parameter IN ('temperature_min', 'temperature_max') AND target.timestamp >= {current_day_start} AND EXISTS (SELECT 1 FROM deduped_gold_view AS source WHERE source.asset_id = target.asset_id AND source.depth = target.depth)"
        spark.sql(delete_min_max)
        logger.info("Deleted all MIN/MAX records for updated assets/depths.")
        # Step 3: Append clean results
        logger.info("Appending deduplicated results to gold table...")
        deduped_df_to_write = deduped_df.select("asset_id", "parameter", "timestamp", "value", "depth")
        write_table(logger, deduped_df_to_write, "append", gold_table)
        logger.info("Successfully appended deduplicated gold records.")
    except Exception as e:
        logger.error(f"Error in deduplicate_gold_table_dts: {e}")
        raise RuntimeError(f"Error in deduplicate_gold_table_dts: {e}")

# COMMAND ----------

def resample_gold_dts(
    spark,
    logger,
    dts_silver_table: str,
    partition_cols: str,
    dts_gold_table: str,
    bucket_size_seconds: int = 3600
) -> bool:
    """Main gold resampling and deduplicated append routine."""
    try:
        logger.info(f"Reading silver table: {dts_silver_table}")
        dts_df = read_table(spark, logger, dts_silver_table)
        if dts_df is None or dts_df.count() == 0:
            logger.warning(f"No data found in {dts_silver_table}")
            return False
        logger.info("Bucketing data.")
        bucketed_df = dts_silver_bucket(dts_df, bucket_size_seconds)
        logger.info("Generating bucket times.")
        bucket_times_df = generate_bucket_times(spark, bucket_size_seconds)
        logger.info("Expanding buckets.")
        expanded_buckets = expand_buckets(bucket_times_df, dts_df)
        logger.info("Calculating min/max.")
        min_max_df = dts_get_min_max(bucketed_df, expanded_buckets)
        logger.info("Calculating sum.")
        sum_df = dts_get_sum(bucketed_df, expanded_buckets)
        logger.info("Calculating count.")
        count_df = dts_get_count(bucketed_df, expanded_buckets)
        logger.info("Assembling gold DataFrame.")
        gold_df = dts_get_gold(min_max_df, sum_df, count_df, bucket_size_seconds, spark)
        if gold_df.count() == 0:
            logger.info("Gold DataFrame is empty after aggregations.")
            return False
        logger.info(f"Deduplicating and appending into Delta table: {dts_gold_table}")
        deduplicate_gold_table_dts(spark, logger, dts_gold_table, gold_df, bucket_size_seconds)
        logger.info(f"Summary table for dts :{dts_gold_table} load completed.")
        return True
    except Exception as e:
        logger.error(f"Error in resample_gold_dts: {e}")
        raise
