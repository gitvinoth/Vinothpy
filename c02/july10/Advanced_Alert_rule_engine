# Databricks notebook source
# MAGIC %run /Workspace/Users/vinoth.ravi@bakerhughes.com/utils/logger

# COMMAND ----------

from datetime import datetime
app_name = "Advanced_Rule_pyScriptExe"
job_start_timestamp = datetime.now()
date = job_start_timestamp.strftime("%Y-%m-%d-%H-%M-%S-%f")
logger = configure_logger(app_name, date)

# COMMAND ----------

# List all .py files in the specified ADLS path and execute each using exec
dir_path = "abfss://ccus-dd40badc-44a8-4c24-994a-ab80edc83478-dev@stcoreusnp003d.dfs.core.windows.net/raw_files/ccus/rule/py-scripts/"
files = dbutils.fs.ls(dir_path)
for file_info in files:
    if file_info.name.endswith(".py"):
        script_content = dbutils.fs.head(file_info.path)
        logger.info(f"Executing script: {file_info.path}")
        exec(script_content)

# COMMAND ----------

# MAGIC %md
# MAGIC # ## Data Preperation for **Parameters** to pass into __main()

# COMMAND ----------

from pyspark.sql.functions import col
from typing import Dict, Any
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.functions import when
import time

# COMMAND ----------

# MAGIC %md
# MAGIC Prepare **Current_time_range** Parameter

# COMMAND ----------

# Define start time as 2025-07-10 16:00:00
start_epoch = int(time.mktime(time.strptime("2025-07-10 16:00:00", "%Y-%m-%d %H:%M:%S")))

# COMMAND ----------

start_time = start_epoch + 3600  # 17:00 (5:00 PM)
end_time = start_epoch + 5400    # 17:30 (5:30 PM)

# COMMAND ----------

current_time_range: Dict[str, int] = {"start_time": int(start_time), "end_time": int(end_time)}
current_time_range

# COMMAND ----------

# MAGIC %md
# MAGIC Prepare **Parameter_data** parameter

# COMMAND ----------

# Create 90 minutes of 1-second interval data: 16:00 to 17:30
data = [{"epoch_timestamp": start_epoch + i, "pressure": 2000 + (i % 5)} for i in range(5400)]
# Create DataFrame
test_df = spark.createDataFrame(data)
test_df.orderBy("epoch_timestamp").show(10, truncate=False)

# COMMAND ----------

parameters_data = [{
    "parameter": "pressure",
    "data": {
        "timestamps": [row["epoch_timestamp"] for row in test_df.orderBy("epoch_timestamp").collect()],
        "values": [row["pressure"] for row in test_df.orderBy("epoch_timestamp").collect()]
    }
}]
parameters_data

# COMMAND ----------

# MAGIC %md
# MAGIC Prepare **rule** Parameter

# COMMAND ----------

# MAGIC %md
# MAGIC _**Question: metadata for the rule_id, threshold, baseTimestamp, etc., to be provided by???**_

# COMMAND ----------

# MAGIC %md
# MAGIC _**Answer: These rules metadata will be taken from the bronze_zone.rule table and the file (.py) will be also in the rule table**_

# COMMAND ----------

rule: Dict[str, Any] = {
    "threshold": 10,
    "duration": 3600,
    "metadata": {
        "multiplying_factor": 3
    }
}
rule

# COMMAND ----------

# MAGIC %md
# MAGIC Execute the **__main** from py file

# COMMAND ----------

__main(parameters_data,current_time_range,rule)

# COMMAND ----------

# MAGIC %md
# MAGIC Set Create an **Anamoly** with **_Abnormal_** value

# COMMAND ----------


test_df = test_df.withColumn('pressure', when(test_df['epoch_timestamp'] == 1752166800, 2100).otherwise(test_df['pressure']))
test_df.display()


# COMMAND ----------

parameters_data = [{
    "parameter": "pressure",
    "data": {
        "timestamps": [row["epoch_timestamp"] for row in test_df.orderBy("epoch_timestamp").collect()],
        "values": [row["pressure"] for row in test_df.orderBy("epoch_timestamp").collect()]
    }
}]
parameters_data

# COMMAND ----------

__main(parameters_data,current_time_range,rule)

# COMMAND ----------

def detect_z_score_anomalies(
   spark_df,             # Spark DataFrame with 'epoch_timestamp', 'pressure'
   start_time: int,
   end_time: int,
   duration: int,
   threshold: float,
   logger
):
   logger.info(f"Running Spark-based Z-Score detection")
   logger.info(f"start_time: {start_time}, end_time: {end_time}, duration: {duration}, threshold: {threshold}")
   # Filter only the necessary data: full [start_time - duration, end_time)
   full_range_start = start_time - duration
   logger.info(f"Full range: [{full_range_start}, {end_time}]")
   df = spark_df.filter(
       (F.col("epoch_timestamp") >= full_range_start) & (F.col("epoch_timestamp") < end_time)
   ).select("epoch_timestamp", "pressure")
   df.display()
   # logger.info(f"Dataframe filtered: {df}")
   # Define a 1-hour rolling window (duration seconds) for each row
   w = Window \
       .orderBy("epoch_timestamp") \
       .rangeBetween(-duration, -1)  # previous 'duration' seconds, excluding current row 
   df_with_stats = df.withColumn("mean", F.mean("pressure").over(w)) \
                     .withColumn("stddev", F.stddev("pressure").over(w)) \
                     .withColumn("z_score", (F.col("pressure") - F.col("mean")) / F.col("stddev")) \
                     .withColumn("Alert_Triggered", F.abs(F.col("z_score")) > threshold) \
                     .filter((F.col("epoch_timestamp") >= start_time) & (F.col("epoch_timestamp") < end_time))
   logger.info(f"Z-Score processing complete")
   df_with_stats.display()
   return df_with_stats.select(
       "epoch_timestamp", "pressure", "mean", "stddev", "z_score", "Alert_Triggered"
   )

# COMMAND ----------

duration = 3600
threshold = 10

# COMMAND ----------

detect_z_score_anomalies(
        spark_df=test_df,
        start_time=int(start_time),
        end_time=int(end_time),
        duration=int(duration),
        threshold=float(threshold),
        logger=logger
    )
