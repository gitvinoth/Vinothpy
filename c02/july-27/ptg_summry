# Databricks notebook source
from datetime import datetime
from pyspark.sql import DataFrame
from pyspark.sql.functions import (
    col, floor, explode, sequence, unix_timestamp, current_timestamp, date_trunc, lit,
    struct, min as _min, max as _max, count as _count, sum as _sum, expr, date_add, unix_timestamp, date_trunc
)
from pyspark.sql.window import Window
from pyspark.sql import functions as F

# COMMAND ----------

# MAGIC %run ../utils/logger

# COMMAND ----------

# MAGIC %run ../utils/read_utility

# COMMAND ----------

# MAGIC %run ../utils/write_utility

# COMMAND ----------

try:
    if dbutils:
        pass # pragma: no cover
except NameError:
    from src.utils.write_utility import write_table
    from src.utils.read_utility import read_delta_table, read_table

# COMMAND ----------

def generate_bucket_times_ptg(spark, bucket_size_seconds: int, logger=None) -> DataFrame:
    try:
        current_day_start = unix_timestamp(current_timestamp() - expr("INTERVAL 1 DAY")).cast("long")
        current_ts = unix_timestamp(current_timestamp()).cast("long")
        df = spark.sql("SELECT 1").select(
            explode(sequence(
                current_day_start,
                current_ts,
                lit(bucket_size_seconds).cast("long")
            )).alias("bucket_start")
        )
        if logger: logger.info("Generated bucket times.")
        return df
    except Exception as e:
        if logger: logger.error(f"generate_bucket_times failed: {e}")
        raise

# COMMAND ----------

def expand_buckets_ptg(bucket_times_df: DataFrame, base_df: DataFrame, logger=None) -> DataFrame:
    try:
        asset_df = base_df.select("asset_id").distinct()
        expanded = asset_df.crossJoin(bucket_times_df)
        if logger: logger.info("Expanded buckets.")
        return expanded
    except Exception as e:
        if logger: logger.error(f"expand_buckets failed: {e}")
        raise

# COMMAND ----------

def pt_gauge_bucket(df: DataFrame, bucket_size_seconds: int, logger=None) -> DataFrame:
    try:
        current_day_start = unix_timestamp(current_timestamp() - expr("INTERVAL 1 DAY"))
        current_ts = unix_timestamp(current_timestamp())
        bucketed = df.filter(
            col("asset_id").isNotNull()
            & col("pressure").isNotNull()
            & col("temperature").isNotNull()
            & (col("epoch_timestamp") >= current_day_start)
            & (col("epoch_timestamp") <= current_ts)
        ).withColumn(
            "bucket_start",
            (col("epoch_timestamp") - (col("epoch_timestamp") % bucket_size_seconds))
        )
        if logger: logger.info("Bucketed pt_gauge dataframe.")
        return bucketed
    except Exception as e:
        if logger: logger.error(f"pt_gauge_bucket failed: {e}")
        raise

# COMMAND ----------

def pt_gauge_get_sum(bucketed_df: DataFrame, expanded_buckets: DataFrame, logger=None) -> DataFrame:
    try:
        sum_df = bucketed_df.groupBy("asset_id", "bucket_start").agg(
            _sum("temperature").alias("sum_temperature"),
            _sum("pressure").alias("sum_pressure")
        )
        result = expanded_buckets.join(sum_df, ["asset_id", "bucket_start"], "left")
        if logger: logger.info("Aggregated sum for pt_gauge.")
        return result
    except Exception as e:
        if logger: logger.error(f"pt_gauge_get_sum failed: {e}")
        raise

# COMMAND ----------

def pt_gauge_get_count(bucketed_df: DataFrame, expanded_buckets: DataFrame, logger=None) -> DataFrame:
    try:
        count_df = bucketed_df.groupBy("asset_id", "bucket_start").agg(
            _count("temperature").alias("count_temperature"),
            _count("pressure").alias("count_pressure")
        )
        result = expanded_buckets.join(count_df, ["asset_id", "bucket_start"], "left") \
            .fillna({"count_temperature": 0, "count_pressure": 0})
        if logger: logger.info("Aggregated count for pt_gauge.")
        return result
    except Exception as e:
        if logger: logger.error(f"pt_gauge_get_count failed: {e}")
        raise

# COMMAND ----------

def pt_gauge_get_min_max(bucketed_df: DataFrame, expanded_buckets: DataFrame, logger=None) -> DataFrame:
    try:
        min_max_df = bucketed_df.groupBy("asset_id", "bucket_start").agg(
            _min(struct(col("temperature"), col("epoch_timestamp"))).alias("min_temp_struct"),
            _max(struct(col("temperature"), col("epoch_timestamp"))).alias("max_temp_struct"),
            _min(struct(col("pressure"), col("epoch_timestamp"))).alias("min_pressure_struct"),
            _max(struct(col("pressure"), col("epoch_timestamp"))).alias("max_pressure_struct"),
        ).select(
            "asset_id", "bucket_start",
            col("min_temp_struct.temperature").alias("min_temperature"),
            col("max_temp_struct.temperature").alias("max_temperature"),
            col("min_temp_struct.epoch_timestamp").alias("min_temperature_ts"),
            col("max_temp_struct.epoch_timestamp").alias("max_temperature_ts"),
            col("min_pressure_struct.pressure").alias("min_pressure"),
            col("max_pressure_struct.pressure").alias("max_pressure"),
            col("min_pressure_struct.epoch_timestamp").alias("min_pressure_ts"),
            col("max_pressure_struct.epoch_timestamp").alias("max_pressure_ts"),
        )
        result = expanded_buckets.join(min_max_df, ["asset_id", "bucket_start"], "left")
        if logger: logger.info("Aggregated min/max for pt_gauge.")
        return result
    except Exception as e:
        if logger: logger.error(f"pt_gauge_get_min_max failed: {e}")
        raise

# COMMAND ----------

def pt_gauge_get_gold(min_max_df, sum_df, count_df, spark, logger=None):
    from pyspark.sql import functions as F
    try:
        min_max_with_count = min_max_df.join(
            count_df.select("asset_id", "bucket_start", "count_temperature", "count_pressure"),
            on=["asset_id", "bucket_start"],
            how="left"
        )
        valid_temp = min_max_with_count.filter(F.col("count_temperature") > 0)
        min_temp_df = valid_temp.select(
            "asset_id",
            F.lit("temperature_min").alias("parameter"),
            F.col("min_temperature_ts").cast("long").alias("timestamp"),
            F.col("min_temperature").alias("value"),
            "bucket_start"
        )
        max_temp_df = valid_temp.select(
            "asset_id",
            F.lit("temperature_max").alias("parameter"),
            F.col("max_temperature_ts").cast("long").alias("timestamp"),
            F.col("max_temperature").alias("value"),
            "bucket_start"
        )
        valid_pressure = min_max_with_count.filter(F.col("count_pressure") > 0)
        min_pressure_df = valid_pressure.select(
            "asset_id",
            F.lit("pressure_min").alias("parameter"),
            F.col("min_pressure_ts").cast("long").alias("timestamp"),
            F.col("min_pressure").alias("value"),
            "bucket_start"
        )
        max_pressure_df = valid_pressure.select(
            "asset_id",
            F.lit("pressure_max").alias("parameter"),
            F.col("max_pressure_ts").cast("long").alias("timestamp"),
            F.col("max_pressure").alias("value"),
            "bucket_start"
        )
        sum_temp_df = sum_df.select(
            "asset_id",
            F.lit("temperature_sum").alias("parameter"),
            F.col("bucket_start").cast("long").alias("timestamp"),
            F.col("sum_temperature").alias("value"),
            "bucket_start"
        )
        sum_pressure_df = sum_df.select(
            "asset_id",
            F.lit("pressure_sum").alias("parameter"),
            F.col("bucket_start").cast("long").alias("timestamp"),
            F.col("sum_pressure").alias("value"),
            "bucket_start"
        )
        count_temp_df = count_df.select(
            "asset_id",
            F.lit("temperature_count").alias("parameter"),
            F.col("bucket_start").cast("long").alias("timestamp"),
            F.col("count_temperature").alias("value"),
            "bucket_start"
        )
        count_pressure_df = count_df.select(
            "asset_id",
            F.lit("pressure_count").alias("parameter"),
            F.col("bucket_start").cast("long").alias("timestamp"),
            F.col("count_pressure").alias("value"),
            "bucket_start"
        )
        gold_df = min_temp_df.union(max_temp_df).union(min_pressure_df).union(max_pressure_df)\
            .union(sum_temp_df).union(sum_pressure_df).union(count_temp_df).union(count_pressure_df)\
            .select("asset_id", "parameter", "timestamp", "value", "bucket_start")\
            .filter(
                F.col("value").isNotNull() &
                F.col("timestamp").isNotNull() &
                F.col("asset_id").isNotNull()
            )
        window = Window.partitionBy("asset_id", "parameter", "bucket_start").orderBy(F.col("timestamp").asc())
        gold_df = gold_df.withColumn("rn", F.row_number().over(window)).filter(F.col("rn") == 1).drop("rn")
        if logger: logger.info("Built gold summary DataFrame for pt_gauge.")
        return gold_df.select("asset_id", "parameter", "timestamp", "value", "bucket_start")
    except Exception as e:
        if logger: logger.error(f"pt_gauge_get_gold failed: {e}")
        raise

# COMMAND ----------

def deduplicate_pt_gauge_gold_table(
   spark,
   logger,
   pt_gauge_gold_table: str,
   gold_df: DataFrame,
   bucket_size_seconds: int
) -> None:
   """
   Deduplicate PT Gauge Gold Table:
   1. Align to bucket_start.
   2. Deduplicate min/max/sum/count by bucket.
   3. DELETE:
      - SUM/COUNT: (asset_id, parameter, timestamp)
      - MIN/MAX: (asset_id) for current day
   4. Append deduplicated data into gold table.
   """
   try:
       logger.info("Aligning gold_df to bucket_start...")
       aligned_df = gold_df.withColumn(
           "bucket_start",
           (F.col("timestamp") / bucket_size_seconds).cast("long") * bucket_size_seconds
       )
       def get_deduped(aligned_df, param, is_min=True):
           if "min" in param or "max" in param:
               order = F.col("value").asc() if is_min else F.col("value").desc()
               ts_order = F.col("timestamp").asc() if is_min else F.col("timestamp").desc()
               window = Window.partitionBy("asset_id", "parameter", "bucket_start").orderBy(order, ts_order)
               return aligned_df.filter(F.col("parameter") == param) \
                   .withColumn("rn", F.row_number().over(window)) \
                   .filter(F.col("rn") == 1).drop("rn")
           else:
               return aligned_df.filter(F.col("parameter") == param) \
                   .groupBy("asset_id", "parameter", "bucket_start") \
                   .agg(F.sum("value").alias("value"), F.min("timestamp").alias("timestamp"))
       logger.info("Computing deduplicated DataFrames...")
       min_df = get_deduped(aligned_df, "temperature_min", is_min=True) \
           .unionByName(get_deduped(aligned_df, "pressure_min", is_min=True))
       max_df = get_deduped(aligned_df, "temperature_max", is_min=False) \
           .unionByName(get_deduped(aligned_df, "pressure_max", is_min=False))
       sum_df = get_deduped(aligned_df, "temperature_sum") \
           .unionByName(get_deduped(aligned_df, "pressure_sum"))
       count_df = get_deduped(aligned_df, "temperature_count") \
           .unionByName(get_deduped(aligned_df, "pressure_count"))
       # Combine all
       cols = ["asset_id", "parameter", "timestamp", "value", "bucket_start"]
       deduped_df = min_df.select(*cols).unionByName(max_df.select(*cols)) \
           .unionByName(sum_df.select(*cols)).unionByName(count_df.select(*cols))
       logger.info("Registering temp view for deduped_gold_view...")
       deduped_df.createOrReplaceTempView("deduped_gold_view")
       current_day_start = spark.sql("SELECT unix_timestamp(date_add(current_timestamp(), -1))").collect()[0][0]
       # Step 1: Delete SUM/COUNT from gold table where (asset_id, parameter, timestamp) match
       delete_sum_count = f"""
       DELETE FROM {pt_gauge_gold_table} AS target
       WHERE target.parameter IN (
           'temperature_sum', 'temperature_count',
           'pressure_sum', 'pressure_count'
       )
       AND target.timestamp >= {current_day_start}
       AND EXISTS (
           SELECT 1 FROM deduped_gold_view AS source
           WHERE source.asset_id = target.asset_id
             AND source.parameter = target.parameter
             AND source.timestamp = target.timestamp
       )
       """
       spark.sql(delete_sum_count)
       logger.info("Deleted overlapping SUM/COUNT from gold table.")
       # Step 2: Delete MIN/MAX for today by (asset_id) â€” coarse wipe
       delete_min_max = f"""
       DELETE FROM {pt_gauge_gold_table} AS target
       WHERE target.parameter IN (
           'temperature_min', 'temperature_max',
           'pressure_min', 'pressure_max'
       )
       AND target.timestamp >= {current_day_start}
       AND EXISTS (
           SELECT 1 FROM deduped_gold_view AS source
           WHERE source.asset_id = target.asset_id
       )
       """
       spark.sql(delete_min_max)
       logger.info("Deleted current-day MIN/MAX from gold table.")
       # Step 3: Append deduped result
       logger.info("Appending deduplicated records...")
       final_df = deduped_df.select("asset_id", "parameter", "timestamp", "value")
       final_df.write.format("delta").mode("append").saveAsTable(pt_gauge_gold_table)
       logger.info("Deduplication complete for PT Gauge.")
   except Exception as e:
       logger.error(f"Error in deduplicate_pt_gauge_gold_table: {e}")
       raise RuntimeError(f"Error in deduplicate_pt_gauge_gold_table: {e}")

# COMMAND ----------

def pt_gauge_data_summarize(
    spark,
    logger,
    pt_gauge_silver_table,
    pt_gauge_summary_table,
    bucket_size_seconds: int = 60,
) -> bool:
    try:
        pt_gauge_silver_df = read_table(spark, logger, pt_gauge_silver_table)
        if pt_gauge_silver_df is None or pt_gauge_silver_df.count() == 0:
            logger.info(f"No data found in {pt_gauge_silver_table}")
            return False
        bucketed_df = pt_gauge_bucket(pt_gauge_silver_df, bucket_size_seconds, logger)
        bucket_times_df = generate_bucket_times_ptg(spark, bucket_size_seconds, logger)
        expanded_buckets = expand_buckets_ptg(bucket_times_df, pt_gauge_silver_df, logger)
        min_max_df = pt_gauge_get_min_max(bucketed_df, expanded_buckets, logger)
        sum_df = pt_gauge_get_sum(bucketed_df, expanded_buckets, logger)
        count_df = pt_gauge_get_count(bucketed_df, expanded_buckets, logger)
        gold_df = pt_gauge_get_gold(min_max_df, sum_df, count_df, spark, logger)
        if gold_df.count() == 0:
            logger.info("Gold DataFrame is empty after aggregations.")
            return False
        logger.info(f"Appending into Ptg Summary table : {pt_gauge_summary_table}")
        deduplicate_pt_gauge_gold_table(spark, logger, pt_gauge_summary_table, gold_df, bucket_size_seconds)
        logger.info("Data Load completed.")
        return True
    except Exception as e:
        logger.error(f"Error in pt_gauge_data_summarize(): {e}")
        raise
