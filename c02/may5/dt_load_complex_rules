# Databricks notebook source
from datetime import datetime
from delta.tables import DeltaTable
from pyspark.sql.functions import (
    input_file_name,
    split,
    size,
    lit,
    explode,
    col,
    when,
    concat_ws,
    lower,
    trim,
    udf,
    array,
    expr,
    row_number,
    desc,
    substring,
    concat,
    regexp_replace,
    max,
    explode,
    collect_list,
    monotonically_increasing_id,
    nvl,
    coalesce,
    current_timestamp,
)
from pyspark.sql.window import Window
from pyspark.sql.types import (
    StructType,
    StructField,
    IntegerType,
    StringType,
    ArrayType,
    DoubleType,
)

# COMMAND ----------

# MAGIC %run ../utils/logger

# COMMAND ----------

# MAGIC %run ../utils/read_utility

# COMMAND ----------

# MAGIC %run ../utils/write_utility

# COMMAND ----------

# MAGIC %run ../utils/file_metadata_utility

# COMMAND ----------

from itertools import product
from math import factorial
from pyspark.sql.functions import (
   col, collect_list, row_number, monotonically_increasing_id,
   current_timestamp, count, expr, lit, when, concat_ws
)
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# COMMAND ----------

# Load all rules with multiple conditions from bronze zone
rules_df = spark.read.table("`ccus_dd40badc-44a8-4c24-994a-ab80edc83478_dev_03`.bronze_zone.rules")

# COMMAND ----------

rules_df.display()

# COMMAND ----------

# Step 2: Filter rules having more than one condition per rule_id
multi_condition_rule_ids_df = rules_df.groupBy("rule_id") \
   .count().filter("count > 1") \
   .select("rule_id")
multi_condition_rules_df = rules_df.join(multi_condition_rule_ids_df, on="rule_id", how="inner")

# COMMAND ----------

multi_condition_rules_df.display()

# COMMAND ----------

# Step 3: Group queries by rule_id and sensor_type
grouped_df = multi_condition_rules_df.groupBy("rule_id", "sensor_type") \
   .agg(collect_list("query").alias("queries"))

# COMMAND ----------

grouped_df.display()

# COMMAND ----------

# Step 4: Convert to Pandas for cross-product logic
pandas_df = grouped_df.toPandas()

# COMMAND ----------

# Step 5: Generate all INTERSECT combinations
results = []
for rule_id in pandas_df["rule_id"].unique():
   subset = pandas_df[pandas_df["rule_id"] == rule_id]
   query_map = {row["sensor_type"]: row["queries"] for _, row in subset.iterrows()}
   for combo in product(*query_map.values()):
       cleaned_combo = []
       for q in combo:
           q = q.strip()
           if q.startswith("WITH"):
               cleaned_combo.append(f"SELECT start_time, end_time FROM ({q})")
           else:
               cleaned_combo.append(q)
       combined_query = " INTERSECT ".join(cleaned_combo)
       results.append((rule_id, combined_query))

# COMMAND ----------

print(results)

# COMMAND ----------

# Convert numpy.int32 to native Python int
converted_results = [(int(row[0]), row[1]) for row in results]

# Step 6: Create Spark DataFrame from result
schema = StructType([
   StructField("rule_id", IntegerType(), True),
   StructField("combined_query", StringType(), True),
])
combo_df = spark.createDataFrame(converted_results, schema)
display(combo_df)

# COMMAND ----------

converted_results

# COMMAND ----------

# Step 7: Assign group_id using row_number
window_spec = Window.partitionBy("rule_id").orderBy(monotonically_increasing_id())
combo_df = combo_df.withColumn("group_id", row_number().over(window_spec)) \
                  .withColumn("created_at", current_timestamp())

# COMMAND ----------

# Step 8: Compute group_id as product of factorials
sensor_count_df = multi_condition_rules_df.groupBy("rule_id", "sensor_type") \
   .agg(count("*").alias("sensor_count"))

fact_df = sensor_count_df.withColumn("factorial", expr("factorial(sensor_count)"))
group_fact_df = fact_df.groupBy("rule_id") \
   .agg(expr("aggregate(collect_list(factorial), 1L, (acc, x) -> acc * x)").alias("group_id_math"))

# COMMAND ----------

group_fact_df.display()

# COMMAND ----------

# Step 9: Join factorial-based group_id
final_combined_df = combo_df.join(group_fact_df, on="rule_id", how="left")

# COMMAND ----------

final_combined_df.display()

# COMMAND ----------

# Step 10: Write final results into bronze_type.rules (merge/update or append as needed)
final_combined_df.write.format("delta") \
   .mode("append") \
   .option("mergeSchema", "true") \
   .saveAsTable("bronze_type.rules")
