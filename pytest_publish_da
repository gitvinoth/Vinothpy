import pytest
from pyspark.sql.functions import col, struct
from pyspark.sql.types import (
    StructType,
    StructField,
    StringType,
    ArrayType,
    LongType,
    DoubleType,
    FloatType,
)

from unittest.mock import patch, Mock
from src.etl.dt_publish_das_copy import (
    das_data_filter,
    das_data_window_append,
    das_file_publish,
    #last_window_start_time,
    current_window_start_time,
    das_data_publish,
)
from unittest.mock import patch, MagicMock, mock_open
from pyspark.sql import SparkSession, DataFrame, Row
import logging
import zipfile


def test_das_data_filter(spark, logger):
    data = [
        (
            "freq1",
            "device1",
            "asset1",
            [1609459200, 1609462800, 1609466400],
            [1.0, 2.0, 3.0],
            10.0,
        ),
        (
            "freq2",
            "device2",
            "asset2",
            [1609459200, 1609462800, 1609466400],
            [4.0, 5.0, 6.0],
            20.0,
        ),
    ]
    schema = StructType(
        [
            StructField("freq_band", StringType(), True),
            StructField("device_id", StringType(), True),
            StructField("asset_id", StringType(), True),
            StructField("time", ArrayType(LongType()), True),
            StructField("data", ArrayType(DoubleType()), True),
            StructField("depth", DoubleType(), True),
        ]
    )
    df = spark.createDataFrame(data, schema)
    filtered_df = das_data_filter(spark, logger, 1609459200, 3600, df)
    assert filtered_df is not None
    assert filtered_df.count() == 2


def test_das_data_window_append(spark, logger):
    data = [
        (
            "freq1",
            "device1",
            "asset1",
            [1609459200, 1609462800, 1609466400],
            [1.0, 2.0, 3.0],
            10.0,
        ),
        (
            "freq2",
            "device2",
            "asset2",
            [1609459200, 1609462800, 1609466400],
            [4.0, 5.0, 6.0],
            20.0,
        ),
    ]
    schema = StructType(
        [
            StructField("freq_band", StringType(), True),
            StructField("device_id", StringType(), True),
            StructField("asset_id", StringType(), True),
            StructField("time", ArrayType(LongType()), True),
            StructField("data", ArrayType(DoubleType()), True),
            StructField("depth", DoubleType(), True),
        ]
    )
    df = spark.createDataFrame(data, schema)
    appended_df = das_data_window_append(logger, df, 2)
    assert appended_df is not None
    assert appended_df.count() == 2
    assert "json_output" in appended_df.columns


'''def test_last_window_start_time(spark, logger):
    start_time = last_window_start_time(spark, logger, 1609459200, 3600)
    assert start_time == 1609455600'''


def test_current_window_start_time(spark, logger):
    start_time = current_window_start_time(spark, logger, 1609459200, 3600)
    assert start_time == 1609459200


@patch("src.etl.dt_publish_das_copy.copy_file")
@patch("src.etl.dt_publish_das_copy.os.remove")
@patch("src.etl.dt_publish_das_copy.zipfile.ZipFile")
@patch("src.etl.dt_publish_das_copy.open", new_callable=mock_open)
@patch("src.etl.dt_publish_das_copy.json.dumps")
def test_das_file_publish_success(
    mock_json_dumps,
    mock_open_func,
    mock_zipfile,
    mock_os,
    mock_copy_file,
    spark,
    logger,
):
    time_window = 1800
    freq = "30mins"
    epoch_start_time = 1719569992
    mock_target_file_path = "file://folder/"
    mock_asset_id = "mock asset id"
    mock_device_id = "mock device id"
    mock_json_output = {"mock_key": "mock value"}
    mock_freq_band = "mock freq band"
    mock = MagicMock()
    mock_input_df = MagicMock(spec=DataFrame)
    mock_row = Row(
        freq_band=mock_freq_band,
        json_output=Row(mock_key="mock value"),
        device_id=mock_device_id,
        asset_id=mock_asset_id,
    )
    mock_input_df.collect.return_value = [mock_row]
    mock_json_dumps.return_value = mock_json_output
    mock_copy_file.return_value = mock

    mock_write_file_path = f"{mock_target_file_path}/{mock_asset_id}"
    mock_file_path = (
        f"{mock_write_file_path}/{mock_freq_band}/{epoch_start_time}_{freq}.json"
    )
    mock_cloud_zip_file_path = (
        f"{mock_write_file_path}/{mock_freq_band}/{epoch_start_time}_{freq}.zip"
    )
    mock_local_file_path = (
        f"/{mock_asset_id}_{mock_freq_band}_{epoch_start_time}_{freq}.json"
    )
    mock_zip_file_path = (
        f"/tmp/{mock_asset_id}_{mock_freq_band}_{epoch_start_time}_{freq}.zip"
    )

    das_file_publish(
        logger, mock_input_df, mock_target_file_path, epoch_start_time, time_window
    )

    mock_input_df.collect.assert_called_once()
    mock_json_dumps.assert_called_once()
    mock_json_dumps.assert_any_call(mock_json_output, ensure_ascii=False)
    mock_open_func.assert_any_call(mock_local_file_path, "w")
    mock_file_handle = mock_open_func()
    mock_file_handle.write.assert_any_call(mock_json_output)
    mock_zipfile.assert_any_call(
        mock_zip_file_path, "w", compression=zipfile.ZIP_DEFLATED, compresslevel=1
    )
    mock_os.assert_any_call(mock_local_file_path)
    mock_copy_file.assert_any_call(
        f"file:{mock_zip_file_path}", mock_cloud_zip_file_path
    )
    mock_os.assert_any_call(mock_zip_file_path)


def test_das_data_filter_exception(spark, logger):
    with pytest.raises(Exception) as excinfo:
        # Simulate an error by passing an incorrect type
        df = spark.createDataFrame([], StructType([]))  # empty DataFrame
        das_data_filter(spark, logger, 1609459200, 3600, df)
    assert "[UNRESOLVED_COLUMN.WITHOUT_SUGGESTION]" in str(excinfo.value)


def test_das_data_window_append_exception(spark, logger):
    with pytest.raises(Exception) as excinfo:
        # Simulate an error by passing an incorrect type
        df = spark.createDataFrame([], StructType([]))  # empty DataFrame
        das_data_window_append(logger, df, 2)
    assert "[UNRESOLVED_COLUMN.WITHOUT_SUGGESTION]" in str(excinfo.value)


'''def test_last_window_start_time_exception(spark, logger):
    with pytest.raises(Exception) as excinfo:
        # Simulate an error by passing zero time_window which will lead to ZeroDivisionError
        last_window_start_time(spark, logger, 1609459200, 0)
    assert "local variable 'start_time' referenced before assignment" in str(
        excinfo.value
    )'''


def test_current_window_start_time_exception(spark, logger):
    with pytest.raises(Exception) as excinfo:
        # Simulate an error by passing zero time_window which will lead to ZeroDivisionError
        current_window_start_time(spark, logger, 1609459200, 0)
    assert "local variable 'start_time' referenced before assignment" in str(
        excinfo.value
    )


def test_das_file_publish_exception(spark, logger, tmpdir):
    with pytest.raises(Exception) as excinfo:
        das_file_publish(logger, None, None, 1609459200, 3600)
    assert "'NoneType' object has no attribute 'collect'" in str(excinfo.value)


def test_delete_previous_day_data(spark, logger):
    from datetime import datetime
    with patch.object(spark, "sql") as mock_sql:
        test_time = int(datetime(2024, 4, 10, 0, 15).timestamp())
        delete_previous_day_data(spark, logger, "silver_table", test_time)
        mock_sql.assert_called_once()
        assert "DELETE FROM silver_table" in mock_sql.call_args[0][0]


def test_is_end_of_6th_hour_window_true():
    test_time = int(datetime(2024, 4, 10, 5, 45).timestamp())  # 5:45 AM
    assert is_end_of_6th_hour_window(test_time) is True


def test_is_end_of_6th_hour_window_false():
    test_time = int(datetime(2024, 4, 10, 4, 15).timestamp())  # 4:15 AM
    assert is_end_of_6th_hour_window(test_time) is False


def test_is_end_of_day_true():
    test_time = int(datetime(2024, 4, 10, 23, 45).timestamp())
    assert is_end_of_day(test_time) is True


def test_is_end_of_day_false():
    test_time = int(datetime(2024, 4, 10, 21, 00).timestamp())
    assert is_end_of_day(test_time) is False


def test_get_max_time_of_data(spark):
    mock_df = MagicMock()
    mock_df.agg.return_value.collect.return_value = [[1712475600]]
    with patch.object(spark.read, "table", return_value=mock_df):
        result = get_max_time_of_data(spark, "silver_table")
        assert isinstance(result, datetime)
        assert result.timestamp() == 1712475600


def test_get_cw_das_df_filtered(spark, logger):
    sample_df = MagicMock(spec=DataFrame)
    with patch("src.etl.dt_publish_das_copy.das_data_filter", return_value=sample_df), \
         patch("src.etl.dt_publish_das_copy.das_data_window_append", return_value=sample_df):
        result = get_cw_das_df_filtered(spark, logger, 1609459200, 3600, "silver_table", 2)
        assert result == sample_df 
